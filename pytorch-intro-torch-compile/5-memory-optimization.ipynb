{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Optimization with torch.compile\n",
    "\n",
    "This notebook explores memory optimization techniques when using torch.compile, including memory profiling, gradient checkpointing, and memory-efficient compilation modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.profiler\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from triton.testing import do_bench\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in GB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**3\n",
    "    return 0\n",
    "\n",
    "def memory_profile(func, *args, **kwargs):\n",
    "    \"\"\"Profile memory usage of a function\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    initial_memory = get_memory_usage()\n",
    "    result = func(*args, **kwargs)\n",
    "    peak_memory = get_memory_usage()\n",
    "    \n",
    "    return result, peak_memory - initial_memory\n",
    "\n",
    "def compare_memory_usage(eager_fn, compiled_fn, *args, **kwargs):\n",
    "    \"\"\"Compare memory usage between eager and compiled modes\"\"\"\n",
    "    _, eager_memory = memory_profile(eager_fn, *args, **kwargs)\n",
    "    _, compiled_memory = memory_profile(compiled_fn, *args, **kwargs)\n",
    "    \n",
    "    print(f\"Eager mode memory: {eager_memory:.3f} GB\")\n",
    "    print(f\"Compiled mode memory: {compiled_memory:.3f} GB\")\n",
    "    print(f\"Memory reduction: {(eager_memory - compiled_memory) / eager_memory * 100:.1f}%\")\n",
    "    \n",
    "    return eager_memory, compiled_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MemoryEfficientModel(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=2048, num_layers=8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(hidden_dim, 10)\n",
    "        self.use_checkpointing = False\n",
    "    \n",
    "    def enable_gradient_checkpointing(self):\n",
    "        self.use_checkpointing = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_checkpointing and self.training:\n",
    "                x = checkpoint(layer, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# Create models for comparison\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_eager = MemoryEfficientModel().to(device)\n",
    "model_compiled = torch.compile(MemoryEfficientModel().to(device), mode=\"reduce-overhead\")\n",
    "model_checkpointed = MemoryEfficientModel().to(device)\n",
    "model_checkpointed.enable_gradient_checkpointing()\n",
    "model_checkpointed_compiled = torch.compile(model_checkpointed, mode=\"reduce-overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_step(model, x, target, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = torch.nn.functional.cross_entropy(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Create test data\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, 1024).to(device)\n",
    "target = torch.randint(0, 10, (batch_size,)).to(device)\n",
    "\n",
    "# Test different configurations\n",
    "configs = [\n",
    "    (\"Eager\", model_eager),\n",
    "    (\"Compiled\", model_compiled),\n",
    "    (\"Checkpointed\", model_checkpointed),\n",
    "    (\"Checkpointed + Compiled\", model_checkpointed_compiled)\n",
    "]\n",
    "\n",
    "memory_results = {}\n",
    "for name, model in configs:\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    def train_fn():\n",
    "        return train_step(model, x, target, optimizer)\n",
    "    \n",
    "    _, memory_used = memory_profile(train_fn)\n",
    "    memory_results[name] = memory_used\n",
    "    print(f\"{name}: {memory_used:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Memory Profiling with PyTorch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def profile_memory_timeline(model, x, target):\n",
    "    \"\"\"Profile memory usage over time during training\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        profile_memory=True,\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for _ in range(3):\n",
    "            train_step(model, x, target, optimizer)\n",
    "    \n",
    "    # Print memory summary\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"cuda_memory_usage\", \n",
    "        row_limit=10\n",
    "    ))\n",
    "\n",
    "print(\"Memory profiling - Eager mode:\")\n",
    "profile_memory_timeline(model_eager, x, target)\n",
    "\n",
    "print(\"\\nMemory profiling - Compiled mode:\")\n",
    "profile_memory_timeline(model_compiled, x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Compilation Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test different compilation modes for memory efficiency\n",
    "base_model = MemoryEfficientModel().to(device)\n",
    "compilation_modes = {\n",
    "    \"default\": torch.compile(base_model, mode=\"default\"),\n",
    "    \"reduce-overhead\": torch.compile(base_model, mode=\"reduce-overhead\"),\n",
    "    \"max-autotune\": torch.compile(base_model, mode=\"max-autotune\")\n",
    "}\n",
    "\n",
    "mode_memory_results = {}\n",
    "for mode_name, compiled_model in compilation_modes.items():\n",
    "    compiled_model.train()\n",
    "    optimizer = torch.optim.Adam(compiled_model.parameters(), lr=0.001)\n",
    "    \n",
    "    def train_fn():\n",
    "        return train_step(compiled_model, x, target, optimizer)\n",
    "    \n",
    "    _, memory_used = memory_profile(train_fn)\n",
    "    mode_memory_results[mode_name] = memory_used\n",
    "    print(f\"Mode '{mode_name}': {memory_used:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Memory usage by configuration\n",
    "configs = list(memory_results.keys())\n",
    "memory_usage = list(memory_results.values())\n",
    "\n",
    "bars1 = ax1.bar(configs, memory_usage, color=['red', 'blue', 'green', 'purple'])\n",
    "ax1.set_ylabel('Memory Usage (GB)')\n",
    "ax1.set_title('Memory Usage by Configuration')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars1, memory_usage):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Memory usage by compilation mode\n",
    "modes = list(mode_memory_results.keys())\n",
    "mode_memory = list(mode_memory_results.values())\n",
    "\n",
    "bars2 = ax2.bar(modes, mode_memory, color=['orange', 'cyan', 'magenta'])\n",
    "ax2.set_ylabel('Memory Usage (GB)')\n",
    "ax2.set_title('Memory Usage by Compilation Mode')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars2, mode_memory):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Gradient Checkpointing**: Trades computation for memory by not storing intermediate activations\n",
    "2. **Compilation Modes**: Different modes have varying memory footprints\n",
    "3. **Memory Profiling**: Essential for identifying memory bottlenecks\n",
    "4. **Combined Approaches**: Checkpointing + compilation can provide best memory efficiency\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- Use `reduce-overhead` mode for memory-constrained environments\n",
    "- Enable gradient checkpointing for very deep models\n",
    "- Profile regularly to monitor memory usage patterns\n",
    "- Consider mixed precision training for further memory savings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}