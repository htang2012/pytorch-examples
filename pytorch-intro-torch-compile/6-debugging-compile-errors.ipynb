{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging torch.compile Errors and Fallbacks\n",
    "\n",
    "This notebook provides comprehensive guidance on debugging compilation errors, understanding fallback mechanisms, and troubleshooting common issues with torch.compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._dynamo as dynamo\n",
    "import logging\n",
    "import warnings\n",
    "from torch._dynamo.utils import CompileProfiler\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Debugging Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enable detailed logging\n",
    "torch._dynamo.config.log_level = logging.INFO\n",
    "torch._dynamo.config.verbose = True\n",
    "\n",
    "# Configure debugging flags\n",
    "torch._dynamo.config.suppress_errors = False\n",
    "torch._dynamo.config.print_specializations = True\n",
    "\n",
    "@contextmanager\n",
    "def debug_compile():\n",
    "    \"\"\"Context manager for enhanced debugging\"\"\"\n",
    "    original_log_level = torch._dynamo.config.log_level\n",
    "    original_verbose = torch._dynamo.config.verbose\n",
    "    \n",
    "    torch._dynamo.config.log_level = logging.DEBUG\n",
    "    torch._dynamo.config.verbose = True\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        torch._dynamo.config.log_level = original_log_level\n",
    "        torch._dynamo.config.verbose = original_verbose\n",
    "\n",
    "def compilation_counter():\n",
    "    \"\"\"Track compilation statistics\"\"\"\n",
    "    stats = dynamo.utils.counters[\"frames\"]\n",
    "    print(\"Compilation Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Reset counters\n",
    "    dynamo.utils.counters.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Problematic Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ProblematicModel(nn.Module):\n",
    "    \"\"\"Model with patterns that can cause compilation issues\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "        self.dynamic_list = []\n",
    "    \n",
    "    def forward_with_python_list(self, x):\n",
    "        \"\"\"Issue: Dynamic Python list modification\"\"\"\n",
    "        result = self.linear(x)\n",
    "        self.dynamic_list.append(result.sum().item())  # Problematic\n",
    "        return result\n",
    "    \n",
    "    def forward_with_control_flow(self, x):\n",
    "        \"\"\"Issue: Data-dependent control flow\"\"\"\n",
    "        result = self.linear(x)\n",
    "        if result.sum() > 0:  # Data-dependent condition\n",
    "            return result * 2\n",
    "        else:\n",
    "            return result * -1\n",
    "    \n",
    "    def forward_with_external_function(self, x):\n",
    "        \"\"\"Issue: External function call\"\"\"\n",
    "        import numpy as np  # Import inside function\n",
    "        result = self.linear(x)\n",
    "        # Convert to numpy and back (unsupported pattern)\n",
    "        np_result = result.detach().cpu().numpy()\n",
    "        return torch.from_numpy(np.sin(np_result)).to(x.device)\n",
    "    \n",
    "    def forward_good(self, x):\n",
    "        \"\"\"Good pattern: Pure tensor operations\"\"\"\n",
    "        result = self.linear(x)\n",
    "        return torch.sin(result)  # Use torch.sin instead of numpy\n",
    "\n",
    "model = ProblematicModel()\n",
    "x = torch.randn(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Compilation Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def test_compilation(model_fn, x, description):\n",
    "    \"\"\"Test compilation and catch/analyze errors\"\"\"\n",
    "    print(f\"\\n=== Testing: {description} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Reset dynamo state\n",
    "        dynamo.reset()\n",
    "        \n",
    "        # Compile the function\n",
    "        compiled_fn = torch.compile(model_fn)\n",
    "        \n",
    "        # Try to execute\n",
    "        with debug_compile():\n",
    "            result = compiled_fn(x)\n",
    "        \n",
    "        print(f\"✅ Success: {description}\")\n",
    "        compilation_counter()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {description}\")\n",
    "        print(f\"   Exception: {type(e).__name__}: {e}\")\n",
    "        \n",
    "        # Print compilation statistics even on failure\n",
    "        compilation_counter()\n",
    "        \n",
    "        # Analyze the error\n",
    "        if \"graph break\" in str(e).lower():\n",
    "            print(\"   This is likely a graph break issue\")\n",
    "        elif \"unsupported\" in str(e).lower():\n",
    "            print(\"   This operation is not supported by the compiler\")\n",
    "        elif \"dynamic\" in str(e).lower():\n",
    "            print(\"   This might be related to dynamic shapes or control flow\")\n",
    "\n",
    "# Test different patterns\n",
    "test_compilation(model.forward_with_python_list, x, \"Python list modification\")\n",
    "test_compilation(model.forward_with_control_flow, x, \"Data-dependent control flow\")\n",
    "test_compilation(model.forward_with_external_function, x, \"External function call\")\n",
    "test_compilation(model.forward_good, x, \"Good pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Graph Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_graph_breaks(model_fn, x, description):\n",
    "    \"\"\"Analyze and explain graph breaks\"\"\"\n",
    "    print(f\"\\n=== Graph Break Analysis: {description} ===\")\n",
    "    \n",
    "    # Enable graph break debugging\n",
    "    dynamo.reset()\n",
    "    \n",
    "    # Capture graph breaks\n",
    "    graph_breaks = []\n",
    "    \n",
    "    def graph_break_handler(frame, event, arg):\n",
    "        if event == 'call':\n",
    "            graph_breaks.append(f\"Call to {frame.f_code.co_name}\")\n",
    "        return graph_break_handler\n",
    "    \n",
    "    try:\n",
    "        compiled_fn = torch.compile(model_fn, fullgraph=False)  # Allow graph breaks\n",
    "        result = compiled_fn(x)\n",
    "        \n",
    "        # Check for graph breaks in compilation\n",
    "        break_reasons = dynamo.utils.counters.get(\"graph_break\", {})\n",
    "        if break_reasons:\n",
    "            print(\"Graph breaks detected:\")\n",
    "            for reason, count in break_reasons.items():\n",
    "                print(f\"  {reason}: {count} times\")\n",
    "        else:\n",
    "            print(\"No graph breaks detected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during compilation: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        dynamo.utils.counters.clear()\n",
    "\n",
    "# Analyze graph breaks for different patterns\n",
    "analyze_graph_breaks(model.forward_with_python_list, x, \"Python list modification\")\n",
    "analyze_graph_breaks(model.forward_with_control_flow, x, \"Data-dependent control flow\")\n",
    "analyze_graph_breaks(model.forward_with_external_function, x, \"External function call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback Mechanisms and Workarounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FallbackDemoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "    \n",
    "    def problematic_forward(self, x):\n",
    "        \"\"\"Method that will trigger fallbacks\"\"\"\n",
    "        result = self.linear(x)\n",
    "        \n",
    "        # This will cause a graph break\n",
    "        if result.sum() > 0:\n",
    "            print(\"Positive sum detected\")  # Side effect\n",
    "            return result * 2\n",
    "        return result\n",
    "    \n",
    "    def workaround_forward(self, x):\n",
    "        \"\"\"Workaround version\"\"\"\n",
    "        result = self.linear(x)\n",
    "        \n",
    "        # Use torch.where instead of if-else\n",
    "        condition = result.sum() > 0\n",
    "        return torch.where(condition, result * 2, result)\n",
    "\n",
    "fallback_model = FallbackDemoModel()\n",
    "\n",
    "print(\"Testing fallback behavior:\")\n",
    "\n",
    "# Test with different configurations\n",
    "configs = [\n",
    "    (\"No compilation\", lambda fn: fn),\n",
    "    (\"Compiled (allow breaks)\", lambda fn: torch.compile(fn, fullgraph=False)),\n",
    "    (\"Compiled (require fullgraph)\", lambda fn: torch.compile(fn, fullgraph=True))\n",
    "]\n",
    "\n",
    "for config_name, compile_fn in configs:\n",
    "    print(f\"\\n--- {config_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        dynamo.reset()\n",
    "        compiled_model = compile_fn(fallback_model.problematic_forward)\n",
    "        result = compiled_model(x)\n",
    "        print(f\"✅ Success with {config_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed with {config_name}: {e}\")\n",
    "        \n",
    "        # Try workaround\n",
    "        try:\n",
    "            dynamo.reset()\n",
    "            compiled_workaround = compile_fn(fallback_model.workaround_forward)\n",
    "            result = compiled_workaround(x)\n",
    "            print(f\"✅ Workaround successful\")\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Workaround also failed: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Impact of Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from triton.testing import do_bench\n",
    "\n",
    "def benchmark_fallback_impact(model_fn, workaround_fn, x):\n",
    "    \"\"\"Benchmark performance impact of fallbacks\"\"\"\n",
    "    \n",
    "    # Eager mode\n",
    "    eager_time = do_bench(lambda: model_fn(x))\n",
    "    \n",
    "    # Compiled with fallbacks\n",
    "    dynamo.reset()\n",
    "    compiled_with_breaks = torch.compile(model_fn, fullgraph=False)\n",
    "    compiled_time = do_bench(lambda: compiled_with_breaks(x))\n",
    "    \n",
    "    # Compiled workaround (should be faster)\n",
    "    dynamo.reset()\n",
    "    compiled_workaround = torch.compile(workaround_fn, fullgraph=True)\n",
    "    workaround_time = do_bench(lambda: compiled_workaround(x))\n",
    "    \n",
    "    print(f\"Eager mode: {eager_time:.4f} ms\")\n",
    "    print(f\"Compiled with breaks: {compiled_time:.4f} ms\")\n",
    "    print(f\"Compiled workaround: {workaround_time:.4f} ms\")\n",
    "    \n",
    "    print(f\"\\nSpeedup analysis:\")\n",
    "    print(f\"Compiled vs Eager: {eager_time/compiled_time:.2f}x\")\n",
    "    print(f\"Workaround vs Eager: {eager_time/workaround_time:.2f}x\")\n",
    "    print(f\"Workaround vs Compiled: {compiled_time/workaround_time:.2f}x\")\n",
    "\n",
    "# Run performance comparison\n",
    "print(\"Performance impact of graph breaks:\")\n",
    "benchmark_fallback_impact(\n",
    "    fallback_model.problematic_forward,\n",
    "    fallback_model.workaround_forward,\n",
    "    x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Tools and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CompilationDebugger:\n",
    "    \"\"\"Utility class for debugging compilation issues\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_error(error_msg):\n",
    "        \"\"\"Provide explanations for common error patterns\"\"\"\n",
    "        explanations = {\n",
    "            \"graph break\": \"The compiler encountered an operation it cannot handle and fell back to eager mode.\",\n",
    "            \"dynamic shape\": \"The tensor shape varies at runtime, making compilation difficult.\",\n",
    "            \"unsupported operator\": \"This PyTorch operation is not yet supported by the compiler.\",\n",
    "            \"control flow\": \"Data-dependent branching cannot be compiled efficiently.\",\n",
    "            \"side effect\": \"Operations with side effects (like print) cause graph breaks.\",\n",
    "            \"python container\": \"Modifying Python lists/dicts during forward pass is not supported.\"\n",
    "        }\n",
    "        \n",
    "        error_lower = error_msg.lower()\n",
    "        for pattern, explanation in explanations.items():\n",
    "            if pattern in error_lower:\n",
    "                print(f\"Likely issue: {explanation}\")\n",
    "                return\n",
    "        \n",
    "        print(\"Error pattern not recognized. Check torch._dynamo documentation.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def suggest_workarounds(error_msg):\n",
    "        \"\"\"Suggest workarounds for common issues\"\"\"\n",
    "        suggestions = {\n",
    "            \"graph break\": [\n",
    "                \"Use torch operations instead of Python operations\",\n",
    "                \"Move side effects outside the compiled function\",\n",
    "                \"Use torch.where instead of if-else statements\"\n",
    "            ],\n",
    "            \"dynamic shape\": [\n",
    "                \"Use torch._dynamo.mark_dynamic to mark dynamic dimensions\",\n",
    "                \"Pad tensors to fixed sizes\",\n",
    "                \"Use bucketing for similar-sized inputs\"\n",
    "            ],\n",
    "            \"unsupported operator\": [\n",
    "                \"Use alternative PyTorch operations\",\n",
    "                \"Implement custom operations using torch.library\",\n",
    "                \"Exclude the operation from compilation\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        error_lower = error_msg.lower()\n",
    "        for pattern, workarounds in suggestions.items():\n",
    "            if pattern in error_lower:\n",
    "                print(f\"Suggested workarounds:\")\n",
    "                for i, workaround in enumerate(workarounds, 1):\n",
    "                    print(f\"  {i}. {workaround}\")\n",
    "                return\n",
    "        \n",
    "        print(\"No specific workarounds available. Try simplifying the model.\")\n",
    "\n",
    "# Example usage\n",
    "debugger = CompilationDebugger()\n",
    "print(\"Example error analysis:\")\n",
    "debugger.explain_error(\"Encountered graph break due to unsupported control flow\")\n",
    "debugger.suggest_workarounds(\"Encountered graph break due to unsupported control flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### ✅ Do:\n",
    "1. Use pure tensor operations when possible\n",
    "2. Replace data-dependent control flow with `torch.where`\n",
    "3. Move side effects (print, logging) outside compiled functions\n",
    "4. Use torch operations instead of numpy/python equivalents\n",
    "5. Test with `fullgraph=True` to catch graph breaks early\n",
    "\n",
    "### ❌ Avoid:\n",
    "1. Modifying Python containers (lists, dicts) in forward pass\n",
    "2. Data-dependent control flow (if statements on tensor values)\n",
    "3. Side effects like print statements or file I/O\n",
    "4. Converting to/from numpy arrays\n",
    "5. Dynamic imports inside compiled functions\n",
    "\n",
    "### 🔧 Debugging Tools:\n",
    "1. Enable verbose logging with `torch._dynamo.config.verbose = True`\n",
    "2. Use `fullgraph=True` to require full compilation\n",
    "3. Check `torch._dynamo.utils.counters` for statistics\n",
    "4. Use `torch._dynamo.explain()` for detailed analysis\n",
    "5. Profile with PyTorch profiler for performance insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}