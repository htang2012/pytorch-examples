{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Compiler Exploration\n",
    "\n",
    "This notebook provides an interactive environment to explore torch.compile internals, including TorchDynamo, AOTAutograd, PrimTorch, and TorchInductor components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._dynamo as dynamo\n",
    "from torch._inductor import config as inductor_config\n",
    "from torch.fx import GraphModule\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Any\n",
    "import graphviz\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class InteractiveModelBuilder:\n",
    "    \"\"\"Build models interactively for compiler exploration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.current_dim = None\n",
    "    \n",
    "    def add_linear(self, output_dim: int, activation: str = \"relu\"):\n",
    "        if self.current_dim is None:\n",
    "            raise ValueError(\"Set input dimension first with set_input_dim()\")\n",
    "        \n",
    "        self.layers.append(nn.Linear(self.current_dim, output_dim))\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            self.layers.append(nn.ReLU())\n",
    "        elif activation == \"gelu\":\n",
    "            self.layers.append(nn.GELU())\n",
    "        elif activation == \"tanh\":\n",
    "            self.layers.append(nn.Tanh())\n",
    "        elif activation != \"none\":\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        self.current_dim = output_dim\n",
    "        print(f\"Added Linear({self.layers[-2].in_features} -> {output_dim}) + {activation}\")\n",
    "        return self\n",
    "    \n",
    "    def add_dropout(self, p: float = 0.1):\n",
    "        self.layers.append(nn.Dropout(p))\n",
    "        print(f\"Added Dropout(p={p})\")\n",
    "        return self\n",
    "    \n",
    "    def add_batchnorm(self):\n",
    "        if self.current_dim is None:\n",
    "            raise ValueError(\"Cannot add BatchNorm without knowing current dimension\")\n",
    "        self.layers.append(nn.BatchNorm1d(self.current_dim))\n",
    "        print(f\"Added BatchNorm1d({self.current_dim})\")\n",
    "        return self\n",
    "    \n",
    "    def set_input_dim(self, dim: int):\n",
    "        self.current_dim = dim\n",
    "        print(f\"Set input dimension to {dim}\")\n",
    "        return self\n",
    "    \n",
    "    def build(self):\n",
    "        if not self.layers:\n",
    "            raise ValueError(\"No layers added\")\n",
    "        model = nn.Sequential(*self.layers)\n",
    "        print(f\"\\nBuilt model with {len(self.layers)} layers\")\n",
    "        print(model)\n",
    "        return model\n",
    "    \n",
    "    def reset(self):\n",
    "        self.layers = []\n",
    "        self.current_dim = None\n",
    "        print(\"Model builder reset\")\n",
    "        return self\n",
    "\n",
    "# Interactive model builder\n",
    "builder = InteractiveModelBuilder()\n",
    "\n",
    "# Example: Build a simple model\n",
    "model = (builder\n",
    "         .set_input_dim(784)\n",
    "         .add_linear(512, \"relu\")\n",
    "         .add_dropout(0.2)\n",
    "         .add_linear(256, \"relu\")\n",
    "         .add_batchnorm()\n",
    "         .add_linear(10, \"none\")\n",
    "         .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchDynamo Graph Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GraphExtractor:\n",
    "    \"\"\"Extract and visualize graphs from TorchDynamo\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.extracted_graphs = []\n",
    "        self.original_backend = None\n",
    "    \n",
    "    def extract_graph_backend(self, graph_module: GraphModule, example_inputs):\n",
    "        \"\"\"Custom backend that captures graphs\"\"\"\n",
    "        self.extracted_graphs.append({\n",
    "            'graph_module': graph_module,\n",
    "            'example_inputs': example_inputs,\n",
    "            'nodes': list(graph_module.graph.nodes),\n",
    "            'code': graph_module.code\n",
    "        })\n",
    "        \n",
    "        # Still compile normally\n",
    "        from torch._inductor.compile_fx import compile_fx\n",
    "        return compile_fx(graph_module, example_inputs)\n",
    "    \n",
    "    def extract_graphs(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Extract graphs from model compilation\"\"\"\n",
    "        self.extracted_graphs = []\n",
    "        \n",
    "        # Compile with our custom backend\n",
    "        compiled_model = torch.compile(model, backend=self.extract_graph_backend)\n",
    "        \n",
    "        # Run once to trigger compilation\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(input_tensor)\n",
    "        \n",
    "        return self.extracted_graphs\n",
    "    \n",
    "    def print_graph_info(self, graph_idx: int = 0):\n",
    "        \"\"\"Print information about extracted graph\"\"\"\n",
    "        if not self.extracted_graphs:\n",
    "            print(\"No graphs extracted. Run extract_graphs() first.\")\n",
    "            return\n",
    "        \n",
    "        if graph_idx >= len(self.extracted_graphs):\n",
    "            print(f\"Graph index {graph_idx} out of range. Available: 0-{len(self.extracted_graphs)-1}\")\n",
    "            return\n",
    "        \n",
    "        graph_info = self.extracted_graphs[graph_idx]\n",
    "        nodes = graph_info['nodes']\n",
    "        \n",
    "        print(f\"Graph {graph_idx} Information:\")\n",
    "        print(f\"Total nodes: {len(nodes)}\")\n",
    "        \n",
    "        # Count node types\n",
    "        node_types = {}\n",
    "        for node in nodes:\n",
    "            node_type = node.op\n",
    "            node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "        \n",
    "        print(\"Node types:\")\n",
    "        for node_type, count in node_types.items():\n",
    "            print(f\"  {node_type}: {count}\")\n",
    "        \n",
    "        print(\"\\nNode details:\")\n",
    "        for node in nodes:\n",
    "            if node.op == 'call_function':\n",
    "                print(f\"  {node.name}: {node.target.__name__ if hasattr(node.target, '__name__') else node.target}\")\n",
    "            else:\n",
    "                print(f\"  {node.name}: {node.op}\")\n",
    "    \n",
    "    def visualize_graph(self, graph_idx: int = 0, save_path: str = None):\n",
    "        \"\"\"Visualize graph structure\"\"\"\n",
    "        if not self.extracted_graphs:\n",
    "            print(\"No graphs extracted. Run extract_graphs() first.\")\n",
    "            return\n",
    "        \n",
    "        graph_info = self.extracted_graphs[graph_idx]\n",
    "        nodes = graph_info['nodes']\n",
    "        \n",
    "        # Create directed graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for node in nodes:\n",
    "            label = node.name\n",
    "            if node.op == 'call_function' and hasattr(node.target, '__name__'):\n",
    "                label += f\"\\n{node.target.__name__}\"\n",
    "            G.add_node(node.name, label=label)\n",
    "        \n",
    "        # Add edges\n",
    "        for node in nodes:\n",
    "            for input_node in node.args:\n",
    "                if hasattr(input_node, 'name'):\n",
    "                    G.add_edge(input_node.name, node.name)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Draw graph\n",
    "        nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "                node_size=3000, font_size=8, font_weight='bold',\n",
    "                arrows=True, arrowsize=20, edge_color='gray')\n",
    "        \n",
    "        plt.title(f\"Graph {graph_idx} Structure\")\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "extractor = GraphExtractor()\n",
    "input_tensor = torch.randn(32, 784)\n",
    "\n",
    "# Extract graphs\n",
    "graphs = extractor.extract_graphs(model, input_tensor)\n",
    "print(f\"Extracted {len(graphs)} graphs\")\n",
    "\n",
    "# Analyze first graph\n",
    "extractor.print_graph_info(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOTAutograd Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AOTAutogradExplorer:\n",
    "    \"\"\"Explore AOTAutograd behavior and backward graph generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.forward_graphs = []\n",
    "        self.backward_graphs = []\n",
    "    \n",
    "    def aot_backend(self, graph_module: GraphModule, example_inputs):\n",
    "        \"\"\"Custom AOT backend to capture forward and backward graphs\"\"\"\n",
    "        print(f\"AOT Backend called with graph: {len(list(graph_module.graph.nodes))} nodes\")\n",
    "        \n",
    "        # Store the graph\n",
    "        self.forward_graphs.append({\n",
    "            'graph_module': graph_module,\n",
    "            'nodes': list(graph_module.graph.nodes),\n",
    "            'is_backward': 'backward' in str(graph_module.graph)\n",
    "        })\n",
    "        \n",
    "        # Return the original function for execution\n",
    "        return graph_module.forward\n",
    "    \n",
    "    def explore_aot_autograd(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Explore AOT autograd graph generation\"\"\"\n",
    "        from torch._functorch.aot_autograd import aot_module_simplified\n",
    "        \n",
    "        # Reset storage\n",
    "        self.forward_graphs = []\n",
    "        self.backward_graphs = []\n",
    "        \n",
    "        # Create AOT compiled version\n",
    "        def forward_fn(params, inputs):\n",
    "            return torch.func.functional_call(model, params, inputs)\n",
    "        \n",
    "        # Get model parameters\n",
    "        params = dict(model.named_parameters())\n",
    "        \n",
    "        print(\"Analyzing AOT Autograd behavior...\")\n",
    "        \n",
    "        # Compile with AOT\n",
    "        compiled_model = torch.compile(model, backend=self.aot_backend)\n",
    "        \n",
    "        # Forward pass\n",
    "        input_tensor.requires_grad_(True)\n",
    "        output = compiled_model(input_tensor)\n",
    "        \n",
    "        # Backward pass to generate backward graph\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"Captured {len(self.forward_graphs)} graphs\")\n",
    "        \n",
    "        return self.forward_graphs\n",
    "    \n",
    "    def analyze_gradient_flow(self):\n",
    "        \"\"\"Analyze gradient flow through captured graphs\"\"\"\n",
    "        if not self.forward_graphs:\n",
    "            print(\"No graphs captured. Run explore_aot_autograd() first.\")\n",
    "            return\n",
    "        \n",
    "        for i, graph_info in enumerate(self.forward_graphs):\n",
    "            nodes = graph_info['nodes']\n",
    "            print(f\"\\nGraph {i}:\")\n",
    "            \n",
    "            # Count gradient-related operations\n",
    "            grad_ops = 0\n",
    "            for node in nodes:\n",
    "                if node.op == 'call_function':\n",
    "                    func_name = str(node.target)\n",
    "                    if any(keyword in func_name.lower() for keyword in ['grad', 'backward', 'autograd']):\n",
    "                        grad_ops += 1\n",
    "                        print(f\"  Gradient op: {func_name}\")\n",
    "            \n",
    "            print(f\"  Total gradient operations: {grad_ops}\")\n",
    "            print(f\"  Total nodes: {len(nodes)}\")\n",
    "\n",
    "# Example usage\n",
    "aot_explorer = AOTAutogradExplorer()\n",
    "\n",
    "# Create a model that requires gradients\n",
    "grad_model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "input_tensor = torch.randn(32, 784, requires_grad=True)\n",
    "\n",
    "# Explore AOT autograd\n",
    "aot_graphs = aot_explorer.explore_aot_autograd(grad_model, input_tensor)\n",
    "aot_explorer.analyze_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrimTorch Operation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PrimTorchAnalyzer:\n",
    "    \"\"\"Analyze primitive operations generated by PrimTorch\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.operation_counts = {}\n",
    "        self.primitive_ops = set()\n",
    "    \n",
    "    def analyze_primitives(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Analyze primitive operations in compiled model\"\"\"\n",
    "        from torch._decomp import get_decompositions\n",
    "        from torch._refs import get_decompositions as get_ref_decompositions\n",
    "        \n",
    "        # Get available decompositions\n",
    "        decompositions = get_decompositions([torch.ops.aten])\n",
    "        ref_decompositions = get_ref_decompositions()\n",
    "        \n",
    "        print(f\"Available decompositions: {len(decompositions)}\")\n",
    "        print(f\"Reference decompositions: {len(ref_decompositions)}\")\n",
    "        \n",
    "        # Create custom backend to capture primitive ops\n",
    "        def primitive_analyzer_backend(graph_module, example_inputs):\n",
    "            nodes = list(graph_module.graph.nodes)\n",
    "            \n",
    "            for node in nodes:\n",
    "                if node.op == 'call_function':\n",
    "                    op_name = str(node.target)\n",
    "                    self.operation_counts[op_name] = self.operation_counts.get(op_name, 0) + 1\n",
    "                    \n",
    "                    # Check if it's a primitive operation\n",
    "                    if 'aten' in op_name or 'prims' in op_name:\n",
    "                        self.primitive_ops.add(op_name)\n",
    "            \n",
    "            # Return original compiled function\n",
    "            from torch._inductor.compile_fx import compile_fx\n",
    "            return compile_fx(graph_module, example_inputs)\n",
    "        \n",
    "        # Compile with primitive analysis\n",
    "        compiled_model = torch.compile(model, backend=primitive_analyzer_backend)\n",
    "        \n",
    "        # Execute to trigger compilation\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(input_tensor)\n",
    "        \n",
    "        self.print_primitive_analysis()\n",
    "    \n",
    "    def print_primitive_analysis(self):\n",
    "        \"\"\"Print analysis of primitive operations\"\"\"\n",
    "        print(\"\\n=== Primitive Operation Analysis ===\")\n",
    "        print(f\"Total unique operations: {len(self.operation_counts)}\")\n",
    "        print(f\"Primitive operations: {len(self.primitive_ops)}\")\n",
    "        \n",
    "        # Sort operations by frequency\n",
    "        sorted_ops = sorted(self.operation_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nMost frequent operations:\")\n",
    "        for op, count in sorted_ops[:10]:\n",
    "            is_primitive = \"[PRIMITIVE]\" if op in self.primitive_ops else \"\"\n",
    "            print(f\"  {op}: {count} {is_primitive}\")\n",
    "        \n",
    "        # Categorize operations\n",
    "        categories = {\n",
    "            'aten': [],\n",
    "            'prims': [],\n",
    "            'torch': [],\n",
    "            'other': []\n",
    "        }\n",
    "        \n",
    "        for op in self.operation_counts:\n",
    "            if 'aten' in op:\n",
    "                categories['aten'].append(op)\n",
    "            elif 'prims' in op:\n",
    "                categories['prims'].append(op)\n",
    "            elif 'torch' in op:\n",
    "                categories['torch'].append(op)\n",
    "            else:\n",
    "                categories['other'].append(op)\n",
    "        \n",
    "        print(\"\\nOperation categories:\")\n",
    "        for category, ops in categories.items():\n",
    "            if ops:\n",
    "                print(f\"  {category}: {len(ops)} operations\")\n",
    "    \n",
    "    def compare_before_after_decomposition(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Compare operations before and after decomposition\"\"\"\n",
    "        # Analyze without decomposition\n",
    "        self.operation_counts = {}\n",
    "        self.primitive_ops = set()\n",
    "        \n",
    "        print(\"Analyzing WITHOUT decomposition...\")\n",
    "        basic_compiled = torch.compile(model, backend=\"eager\")\n",
    "        with torch.no_grad():\n",
    "            _ = basic_compiled(input_tensor)\n",
    "        \n",
    "        ops_before = dict(self.operation_counts)\n",
    "        \n",
    "        # Reset and analyze with full compilation\n",
    "        self.operation_counts = {}\n",
    "        self.primitive_ops = set()\n",
    "        \n",
    "        print(\"\\nAnalyzing WITH full compilation...\")\n",
    "        self.analyze_primitives(model, input_tensor)\n",
    "        \n",
    "        ops_after = dict(self.operation_counts)\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(f\"Operations before: {len(ops_before)}\")\n",
    "        print(f\"Operations after: {len(ops_after)}\")\n",
    "        \n",
    "        # Show new operations introduced by decomposition\n",
    "        new_ops = set(ops_after.keys()) - set(ops_before.keys())\n",
    "        if new_ops:\n",
    "            print(f\"\\nNew operations after decomposition ({len(new_ops)}):\")\n",
    "            for op in sorted(new_ops):\n",
    "                print(f\"  {op}: {ops_after[op]}\")\n",
    "\n",
    "# Example usage\n",
    "prim_analyzer = PrimTorchAnalyzer()\n",
    "\n",
    "# Analyze a complex model\n",
    "complex_model = nn.Sequential(\n",
    "    nn.Linear(784, 512),\n",
    "    nn.LayerNorm(512),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "input_tensor = torch.randn(32, 784)\n",
    "prim_analyzer.analyze_primitives(complex_model, input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchInductor Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class InductorExplorer:\n",
    "    \"\"\"Explore TorchInductor code generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.generated_code = []\n",
    "        self.kernel_info = []\n",
    "    \n",
    "    def capture_generated_code(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Capture generated code from TorchInductor\"\"\"\n",
    "        import torch._inductor.config as inductor_config\n",
    "        \n",
    "        # Enable debug mode to capture generated code\n",
    "        old_debug = inductor_config.debug\n",
    "        old_trace = inductor_config.trace.enabled\n",
    "        \n",
    "        inductor_config.debug = True\n",
    "        inductor_config.trace.enabled = True\n",
    "        \n",
    "        try:\n",
    "            # Create temporary directory for generated code\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                inductor_config.output_code = True\n",
    "                inductor_config.debug_dir = temp_dir\n",
    "                \n",
    "                # Compile model\n",
    "                compiled_model = torch.compile(model, mode=\"max-autotune\")\n",
    "                \n",
    "                # Execute to trigger code generation\n",
    "                with torch.no_grad():\n",
    "                    output = compiled_model(input_tensor)\n",
    "                \n",
    "                # Collect generated files\n",
    "                generated_files = []\n",
    "                for root, dirs, files in os.walk(temp_dir):\n",
    "                    for file in files:\n",
    "                        if file.endswith(('.py', '.cpp', '.cu')):\n",
    "                            file_path = os.path.join(root, file)\n",
    "                            try:\n",
    "                                with open(file_path, 'r') as f:\n",
    "                                    content = f.read()\n",
    "                                generated_files.append({\n",
    "                                    'filename': file,\n",
    "                                    'content': content,\n",
    "                                    'size': len(content)\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"Could not read {file}: {e}\")\n",
    "                \n",
    "                self.generated_code = generated_files\n",
    "                \n",
    "        finally:\n",
    "            # Restore original config\n",
    "            inductor_config.debug = old_debug\n",
    "            inductor_config.trace.enabled = old_trace\n",
    "        \n",
    "        print(f\"Captured {len(self.generated_code)} generated files\")\n",
    "        for file_info in self.generated_code:\n",
    "            print(f\"  {file_info['filename']}: {file_info['size']} characters\")\n",
    "    \n",
    "    def analyze_generated_kernels(self):\n",
    "        \"\"\"Analyze generated GPU kernels\"\"\"\n",
    "        if not self.generated_code:\n",
    "            print(\"No generated code available. Run capture_generated_code() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n=== Generated Kernel Analysis ===\")\n",
    "        \n",
    "        for file_info in self.generated_code:\n",
    "            content = file_info['content']\n",
    "            filename = file_info['filename']\n",
    "            \n",
    "            print(f\"\\nFile: {filename}\")\n",
    "            \n",
    "            # Count different types of operations\n",
    "            triton_kernels = content.count('@triton.jit')\n",
    "            cuda_kernels = content.count('__global__')\n",
    "            cpu_kernels = content.count('def cpp_fused')\n",
    "            \n",
    "            print(f\"  Triton kernels: {triton_kernels}\")\n",
    "            print(f\"  CUDA kernels: {cuda_kernels}\")\n",
    "            print(f\"  CPU kernels: {cpu_kernels}\")\n",
    "            \n",
    "            # Look for specific optimizations\n",
    "            if 'vectorized' in content.lower():\n",
    "                print(f\"  ✓ Vectorization detected\")\n",
    "            if 'fused' in content.lower():\n",
    "                print(f\"  ✓ Operator fusion detected\")\n",
    "            if 'tl.load' in content or 'tl.store' in content:\n",
    "                print(f\"  ✓ Triton memory operations detected\")\n",
    "    \n",
    "    def show_sample_kernel(self, file_index: int = 0, max_lines: int = 50):\n",
    "        \"\"\"Show sample generated kernel code\"\"\"\n",
    "        if not self.generated_code or file_index >= len(self.generated_code):\n",
    "            print(\"No code available or invalid index\")\n",
    "            return\n",
    "        \n",
    "        file_info = self.generated_code[file_index]\n",
    "        content = file_info['content']\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        print(f\"\\n=== Sample from {file_info['filename']} ===\")\n",
    "        print(f\"Total lines: {len(lines)}\")\n",
    "        print(f\"Showing first {min(max_lines, len(lines))} lines:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, line in enumerate(lines[:max_lines], 1):\n",
    "            print(f\"{i:3d}: {line}\")\n",
    "        \n",
    "        if len(lines) > max_lines:\n",
    "            print(f\"... ({len(lines) - max_lines} more lines)\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    def compare_compilation_modes(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Compare code generation across different compilation modes\"\"\"\n",
    "        modes = [\"default\", \"reduce-overhead\", \"max-autotune\"]\n",
    "        mode_results = {}\n",
    "        \n",
    "        for mode in modes:\n",
    "            print(f\"\\nAnalyzing mode: {mode}\")\n",
    "            \n",
    "            # Reset state\n",
    "            torch._dynamo.reset()\n",
    "            self.generated_code = []\n",
    "            \n",
    "            # Compile with specific mode\n",
    "            compiled_model = torch.compile(model, mode=mode)\n",
    "            \n",
    "            # Execute\n",
    "            with torch.no_grad():\n",
    "                _ = compiled_model(input_tensor)\n",
    "            \n",
    "            # Get compilation stats\n",
    "            stats = torch._dynamo.utils.counters\n",
    "            mode_results[mode] = {\n",
    "                'frames_ok': stats.get('frames', {}).get('ok', 0),\n",
    "                'graph_breaks': sum(stats.get('graph_break', {}).values()),\n",
    "            }\n",
    "        \n",
    "        print(\"\\n=== Mode Comparison ===\")\n",
    "        for mode, stats in mode_results.items():\n",
    "            print(f\"{mode}:\")\n",
    "            print(f\"  Successful frames: {stats['frames_ok']}\")\n",
    "            print(f\"  Graph breaks: {stats['graph_breaks']}\")\n",
    "\n",
    "# Example usage\n",
    "inductor_explorer = InductorExplorer()\n",
    "\n",
    "# Create a model with various operations\n",
    "inductor_model = nn.Sequential(\n",
    "    nn.Linear(784, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "input_tensor = torch.randn(64, 784)\n",
    "\n",
    "# Capture and analyze generated code\n",
    "inductor_explorer.capture_generated_code(inductor_model, input_tensor)\n",
    "inductor_explorer.analyze_generated_kernels()\n",
    "\n",
    "# Show sample generated code\n",
    "if inductor_explorer.generated_code:\n",
    "    inductor_explorer.show_sample_kernel(0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Compilation Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CompilationDashboard:\n",
    "    \"\"\"Interactive dashboard for exploring compilation behavior\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_builder = InteractiveModelBuilder()\n",
    "        self.graph_extractor = GraphExtractor()\n",
    "        self.aot_explorer = AOTAutogradExplorer()\n",
    "        self.prim_analyzer = PrimTorchAnalyzer()\n",
    "        self.inductor_explorer = InductorExplorer()\n",
    "    \n",
    "    def full_analysis(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Run complete compilation analysis pipeline\"\"\"\n",
    "        print(\"🔍 TORCH.COMPILE FULL ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. Graph extraction\n",
    "        print(\"\\n1. 📊 Extracting computation graphs...\")\n",
    "        graphs = self.graph_extractor.extract_graphs(model, input_tensor)\n",
    "        self.graph_extractor.print_graph_info(0)\n",
    "        \n",
    "        # 2. AOT Autograd analysis\n",
    "        print(\"\\n2. 🔄 Analyzing AOT Autograd...\")\n",
    "        aot_graphs = self.aot_explorer.explore_aot_autograd(model, input_tensor.clone().detach().requires_grad_(True))\n",
    "        self.aot_explorer.analyze_gradient_flow()\n",
    "        \n",
    "        # 3. Primitive operation analysis\n",
    "        print(\"\\n3. 🔧 Analyzing primitive operations...\")\n",
    "        self.prim_analyzer.analyze_primitives(model, input_tensor)\n",
    "        \n",
    "        # 4. Code generation analysis\n",
    "        print(\"\\n4. ⚡ Analyzing generated code...\")\n",
    "        self.inductor_explorer.capture_generated_code(model, input_tensor)\n",
    "        self.inductor_explorer.analyze_generated_kernels()\n",
    "        \n",
    "        print(\"\\n✅ Analysis complete!\")\n",
    "        \n",
    "        return {\n",
    "            'graphs': graphs,\n",
    "            'aot_graphs': aot_graphs,\n",
    "            'primitives': dict(self.prim_analyzer.operation_counts),\n",
    "            'generated_code': self.inductor_explorer.generated_code\n",
    "        }\n",
    "    \n",
    "    def interactive_model_builder(self):\n",
    "        \"\"\"Interactive model building session\"\"\"\n",
    "        print(\"🏗️  INTERACTIVE MODEL BUILDER\")\n",
    "        print(\"Available commands:\")\n",
    "        print(\"  .set_input_dim(dim)\")\n",
    "        print(\"  .add_linear(output_dim, activation='relu')\")\n",
    "        print(\"  .add_dropout(p=0.1)\")\n",
    "        print(\"  .add_batchnorm()\")\n",
    "        print(\"  .build()\")\n",
    "        print(\"  .reset()\")\n",
    "        \n",
    "        return self.model_builder\n",
    "    \n",
    "    def quick_comparison(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Quick performance comparison across modes\"\"\"\n",
    "        from triton.testing import do_bench\n",
    "        \n",
    "        print(\"⚡ QUICK PERFORMANCE COMPARISON\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        modes = [\"eager\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
    "        results = {}\n",
    "        \n",
    "        for mode in modes:\n",
    "            if mode == \"eager\":\n",
    "                compiled_model = model\n",
    "            else:\n",
    "                torch._dynamo.reset()\n",
    "                compiled_model = torch.compile(model, mode=mode)\n",
    "            \n",
    "            # Benchmark\n",
    "            time_ms = do_bench(lambda: compiled_model(input_tensor), warmup=50, rep=100)\n",
    "            results[mode] = time_ms\n",
    "            \n",
    "            print(f\"{mode:15s}: {time_ms:8.3f} ms\")\n",
    "        \n",
    "        # Calculate speedups\n",
    "        eager_time = results[\"eager\"]\n",
    "        print(\"\\nSpeedups:\")\n",
    "        for mode in modes[1:]:\n",
    "            speedup = eager_time / results[mode]\n",
    "            print(f\"{mode:15s}: {speedup:8.2f}x\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = CompilationDashboard()\n",
    "\n",
    "# Example: Quick comparison\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "test_input = torch.randn(64, 1024)\n",
    "\n",
    "# Run quick comparison\n",
    "perf_results = dashboard.quick_comparison(test_model, test_input)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 Use dashboard.full_analysis(model, input) for complete analysis\")\n",
    "print(\"🏗️  Use dashboard.interactive_model_builder() for building models\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}