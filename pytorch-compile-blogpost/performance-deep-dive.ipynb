{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Deep Dive: torch.compile Optimization Strategies\n",
    "\n",
    "This notebook provides an in-depth analysis of performance optimization strategies available through torch.compile, including kernel fusion, memory optimization, and hardware-specific tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from triton.testing import do_bench\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "import time\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measurement Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Container for performance metrics\"\"\"\n",
    "    execution_time: float\n",
    "    memory_usage: float\n",
    "    throughput: float  # samples per second\n",
    "    compilation_time: float = 0.0\n",
    "    first_run_time: float = 0.0\n",
    "\n",
    "class AdvancedBenchmarker:\n",
    "    \"\"\"Advanced benchmarking suite with detailed metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, warmup_runs: int = 100, benchmark_runs: int = 1000):\n",
    "        self.warmup_runs = warmup_runs\n",
    "        self.benchmark_runs = benchmark_runs\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def measure_compilation_overhead(self, model: nn.Module, input_data: torch.Tensor, mode: str = \"default\") -> Tuple[float, float]:\n",
    "        \"\"\"Measure compilation time and first execution time\"\"\"\n",
    "        torch._dynamo.reset()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Measure compilation time\n",
    "        start_time = time.perf_counter()\n",
    "        compiled_model = torch.compile(model, mode=mode)\n",
    "        compilation_time = time.perf_counter() - start_time\n",
    "        \n",
    "        # Measure first execution (includes any additional compilation)\n",
    "        start_time = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _ = compiled_model(input_data)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        first_run_time = time.perf_counter() - start_time\n",
    "        \n",
    "        return compilation_time, first_run_time\n",
    "    \n",
    "    def comprehensive_benchmark(self, model: nn.Module, input_data: torch.Tensor, batch_size: int = None) -> Dict[str, PerformanceMetrics]:\n",
    "        \"\"\"Run comprehensive benchmarks across all modes\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = input_data.shape[0] if len(input_data.shape) > 0 else 1\n",
    "        \n",
    "        modes = {\n",
    "            \"eager\": None,\n",
    "            \"default\": \"default\",\n",
    "            \"reduce-overhead\": \"reduce-overhead\",\n",
    "            \"max-autotune\": \"max-autotune\"\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for mode_name, compile_mode in modes.items():\n",
    "            print(f\"Benchmarking {mode_name} mode...\")\n",
    "            \n",
    "            if mode_name == \"eager\":\n",
    "                test_model = model\n",
    "                compilation_time = 0.0\n",
    "                first_run_time = 0.0\n",
    "            else:\n",
    "                compilation_time, first_run_time = self.measure_compilation_overhead(model, input_data, compile_mode)\n",
    "                test_model = torch.compile(model, mode=compile_mode)\n",
    "            \n",
    "            # Measure steady-state performance\n",
    "            exec_time = do_bench(\n",
    "                lambda: test_model(input_data),\n",
    "                warmup=self.warmup_runs,\n",
    "                rep=self.benchmark_runs\n",
    "            )\n",
    "            \n",
    "            # Measure memory usage\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                with torch.no_grad():\n",
    "                    _ = test_model(input_data)\n",
    "                memory_usage = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "            else:\n",
    "                memory_usage = 0.0\n",
    "            \n",
    "            # Calculate throughput\n",
    "            throughput = (batch_size * 1000) / exec_time  # samples per second\n",
    "            \n",
    "            results[mode_name] = PerformanceMetrics(\n",
    "                execution_time=exec_time,\n",
    "                memory_usage=memory_usage,\n",
    "                throughput=throughput,\n",
    "                compilation_time=compilation_time * 1000,  # Convert to ms\n",
    "                first_run_time=first_run_time * 1000  # Convert to ms\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_performance_comparison(self, results: Dict[str, PerformanceMetrics], title: str = \"Performance Comparison\"):\n",
    "        \"\"\"Create comprehensive performance comparison plots\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        modes = list(results.keys())\n",
    "        colors = ['red', 'blue', 'green', 'orange'][:len(modes)]\n",
    "        \n",
    "        # Execution time comparison\n",
    "        exec_times = [results[mode].execution_time for mode in modes]\n",
    "        bars1 = ax1.bar(modes, exec_times, color=colors)\n",
    "        ax1.set_ylabel('Execution Time (ms)')\n",
    "        ax1.set_title('Execution Time by Mode')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for bar, time in zip(bars1, exec_times):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(exec_times)*0.01,\n",
    "                    f'{time:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Throughput comparison\n",
    "        throughputs = [results[mode].throughput for mode in modes]\n",
    "        bars2 = ax2.bar(modes, throughputs, color=colors)\n",
    "        ax2.set_ylabel('Throughput (samples/sec)')\n",
    "        ax2.set_title('Throughput by Mode')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for bar, throughput in zip(bars2, throughputs):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(throughputs)*0.01,\n",
    "                    f'{throughput:.0f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Memory usage comparison\n",
    "        memory_usage = [results[mode].memory_usage for mode in modes]\n",
    "        bars3 = ax3.bar(modes, memory_usage, color=colors)\n",
    "        ax3.set_ylabel('Memory Usage (GB)')\n",
    "        ax3.set_title('Memory Usage by Mode')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for bar, memory in zip(bars3, memory_usage):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(memory_usage)*0.01,\n",
    "                    f'{memory:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Compilation overhead\n",
    "        compile_times = [results[mode].compilation_time for mode in modes if mode != 'eager']\n",
    "        first_run_times = [results[mode].first_run_time for mode in modes if mode != 'eager']\n",
    "        compile_modes = [mode for mode in modes if mode != 'eager']\n",
    "        \n",
    "        x_pos = np.arange(len(compile_modes))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars4a = ax4.bar(x_pos - width/2, compile_times, width, label='Compilation Time', color='lightblue')\n",
    "        bars4b = ax4.bar(x_pos + width/2, first_run_times, width, label='First Run Time', color='lightcoral')\n",
    "        \n",
    "        ax4.set_ylabel('Time (ms)')\n",
    "        ax4.set_title('Compilation Overhead')\n",
    "        ax4.set_xticks(x_pos)\n",
    "        ax4.set_xticklabels(compile_modes, rotation=45)\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print speedup summary\n",
    "        eager_time = results['eager'].execution_time\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SPEEDUP SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        for mode in modes:\n",
    "            if mode != 'eager':\n",
    "                speedup = eager_time / results[mode].execution_time\n",
    "                print(f\"{mode:15s}: {speedup:6.2f}x speedup\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# Initialize benchmarker\n",
    "benchmarker = AdvancedBenchmarker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Fusion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FusionAnalyzer:\n",
    "    \"\"\"Analyze kernel fusion patterns and benefits\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fusion_patterns = []\n",
    "    \n",
    "    def create_fusion_test_models(self):\n",
    "        \"\"\"Create models with different fusion opportunities\"\"\"\n",
    "        \n",
    "        # Model 1: Element-wise operations (high fusion potential)\n",
    "        class ElementWiseFusionModel(nn.Module):\n",
    "            def forward(self, x):\n",
    "                # These operations can be fused into a single kernel\n",
    "                x = torch.relu(x)\n",
    "                x = x + 1.0\n",
    "                x = x * 2.0\n",
    "                x = torch.sigmoid(x)\n",
    "                return x\n",
    "        \n",
    "        # Model 2: Linear + activation fusion\n",
    "        class LinearActivationFusionModel(nn.Module):\n",
    "            def __init__(self, input_dim=1024, hidden_dim=512, output_dim=256):\n",
    "                super().__init__()\n",
    "                self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "                self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Linear + ReLU can be fused\n",
    "                x = F.relu(self.linear1(x))\n",
    "                # Linear + GELU can be fused\n",
    "                x = F.gelu(self.linear2(x))\n",
    "                return x\n",
    "        \n",
    "        # Model 3: Batch norm + activation fusion\n",
    "        class BatchNormFusionModel(nn.Module):\n",
    "            def __init__(self, num_features=512):\n",
    "                super().__init__()\n",
    "                self.bn1 = nn.BatchNorm1d(num_features)\n",
    "                self.bn2 = nn.BatchNorm1d(num_features)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # BatchNorm + ReLU can be fused\n",
    "                x = F.relu(self.bn1(x))\n",
    "                # Multiple batch norms\n",
    "                x = self.bn2(x)\n",
    "                return x\n",
    "        \n",
    "        # Model 4: No fusion opportunities (control)\n",
    "        class NoFusionModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.linear = nn.Linear(1024, 512)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Simple linear operation with minimal fusion opportunity\n",
    "                return self.linear(x)\n",
    "        \n",
    "        return {\n",
    "            \"elementwise_fusion\": ElementWiseFusionModel(),\n",
    "            \"linear_activation_fusion\": LinearActivationFusionModel(),\n",
    "            \"batchnorm_fusion\": BatchNormFusionModel(),\n",
    "            \"no_fusion\": NoFusionModel()\n",
    "        }\n",
    "    \n",
    "    def analyze_fusion_benefits(self, batch_size: int = 64, input_dim: int = 1024):\n",
    "        \"\"\"Analyze performance benefits of different fusion patterns\"\"\"\n",
    "        models = self.create_fusion_test_models()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        fusion_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nAnalyzing {model_name}...\")\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Create appropriate input\n",
    "            if \"batchnorm\" in model_name:\n",
    "                input_tensor = torch.randn(batch_size, 512, device=device)\n",
    "            else:\n",
    "                input_tensor = torch.randn(batch_size, input_dim, device=device)\n",
    "            \n",
    "            # Benchmark this model\n",
    "            results = benchmarker.comprehensive_benchmark(model, input_tensor, batch_size)\n",
    "            fusion_results[model_name] = results\n",
    "            \n",
    "            # Print quick summary\n",
    "            eager_time = results['eager'].execution_time\n",
    "            compiled_time = results['max-autotune'].execution_time\n",
    "            speedup = eager_time / compiled_time\n",
    "            print(f\"  Speedup with max-autotune: {speedup:.2f}x\")\n",
    "        \n",
    "        return fusion_results\n",
    "    \n",
    "    def plot_fusion_comparison(self, fusion_results: Dict):\n",
    "        \"\"\"Plot fusion analysis results\"\"\"\n",
    "        model_names = list(fusion_results.keys())\n",
    "        modes = ['eager', 'default', 'reduce-overhead', 'max-autotune']\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Execution time comparison\n",
    "        width = 0.2\n",
    "        x = np.arange(len(model_names))\n",
    "        \n",
    "        for i, mode in enumerate(modes):\n",
    "            times = [fusion_results[model][mode].execution_time for model in model_names]\n",
    "            ax1.bar(x + i * width, times, width, label=mode, alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Model Type')\n",
    "        ax1.set_ylabel('Execution Time (ms)')\n",
    "        ax1.set_title('Execution Time by Fusion Pattern')\n",
    "        ax1.set_xticks(x + width * 1.5)\n",
    "        ax1.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        # Speedup comparison\n",
    "        speedup_data = []\n",
    "        for model_name in model_names:\n",
    "            eager_time = fusion_results[model_name]['eager'].execution_time\n",
    "            model_speedups = []\n",
    "            for mode in modes[1:]:  # Skip eager\n",
    "                compiled_time = fusion_results[model_name][mode].execution_time\n",
    "                speedup = eager_time / compiled_time\n",
    "                model_speedups.append(speedup)\n",
    "            speedup_data.append(model_speedups)\n",
    "        \n",
    "        speedup_array = np.array(speedup_data).T\n",
    "        \n",
    "        for i, mode in enumerate(modes[1:]):\n",
    "            ax2.bar(x + i * width, speedup_array[i], width, label=mode, alpha=0.8)\n",
    "        \n",
    "        ax2.set_xlabel('Model Type')\n",
    "        ax2.set_ylabel('Speedup (x)')\n",
    "        ax2.set_title('Speedup by Fusion Pattern')\n",
    "        ax2.set_xticks(x + width)\n",
    "        ax2.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=45)\n",
    "        ax2.legend()\n",
    "        ax2.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print fusion analysis summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FUSION ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            eager_time = fusion_results[model_name]['eager'].execution_time\n",
    "            best_compiled_time = min([fusion_results[model_name][mode].execution_time for mode in modes[1:]])\n",
    "            max_speedup = eager_time / best_compiled_time\n",
    "            \n",
    "            print(f\"{model_name:25s}: {max_speedup:6.2f}x max speedup\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Run fusion analysis\n",
    "fusion_analyzer = FusionAnalyzer()\n",
    "fusion_results = fusion_analyzer.analyze_fusion_benefits()\n",
    "fusion_analyzer.plot_fusion_comparison(fusion_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Pattern Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MemoryPatternAnalyzer:\n",
    "    \"\"\"Analyze memory access patterns and optimization opportunities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def create_memory_test_models(self):\n",
    "        \"\"\"Create models with different memory access patterns\"\"\"\n",
    "        \n",
    "        # Model 1: Sequential memory access (cache-friendly)\n",
    "        class SequentialAccessModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.layers = nn.ModuleList([\n",
    "                    nn.Linear(1024, 1024) for _ in range(4)\n",
    "                ])\n",
    "            \n",
    "            def forward(self, x):\n",
    "                for layer in self.layers:\n",
    "                    x = F.relu(layer(x))\n",
    "                return x\n",
    "        \n",
    "        # Model 2: Random memory access (cache-unfriendly)\n",
    "        class RandomAccessModel(nn.Module):\n",
    "            def __init__(self, vocab_size=10000, embed_dim=512):\n",
    "                super().__init__()\n",
    "                self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "                self.linear = nn.Linear(embed_dim, 256)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Random access to embedding table\n",
    "                embedded = self.embedding(x)\n",
    "                return self.linear(embedded.mean(dim=1))\n",
    "        \n",
    "        # Model 3: Memory-intensive operations\n",
    "        class MemoryIntensiveModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.conv1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "                self.conv2 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "                self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "                self.fc = nn.Linear(256, 10)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.conv1(x))\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x = self.pool(x).flatten(1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        # Model 4: In-place operations (memory efficient)\n",
    "        class InPlaceModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.layers = nn.ModuleList([\n",
    "                    nn.Linear(1024, 1024) for _ in range(4)\n",
    "                ])\n",
    "            \n",
    "            def forward(self, x):\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                    x.relu_()  # In-place ReLU\n",
    "                return x\n",
    "        \n",
    "        return {\n",
    "            \"sequential_access\": SequentialAccessModel(),\n",
    "            \"random_access\": RandomAccessModel(),\n",
    "            \"memory_intensive\": MemoryIntensiveModel(),\n",
    "            \"inplace_ops\": InPlaceModel()\n",
    "        }\n",
    "    \n",
    "    def analyze_memory_patterns(self, batch_size: int = 32):\n",
    "        \"\"\"Analyze memory patterns and their optimization potential\"\"\"\n",
    "        models = self.create_memory_test_models()\n",
    "        memory_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nAnalyzing memory pattern: {model_name}\")\n",
    "            \n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Create appropriate input\n",
    "            if model_name == \"random_access\":\n",
    "                input_tensor = torch.randint(0, 9999, (batch_size, 50), device=self.device)\n",
    "            elif model_name == \"memory_intensive\":\n",
    "                input_tensor = torch.randn(batch_size, 64, 32, 32, device=self.device)\n",
    "            else:\n",
    "                input_tensor = torch.randn(batch_size, 1024, device=self.device)\n",
    "            \n",
    "            # Benchmark with detailed memory tracking\n",
    "            results = self.detailed_memory_benchmark(model, input_tensor, batch_size)\n",
    "            memory_results[model_name] = results\n",
    "        \n",
    "        return memory_results\n",
    "    \n",
    "    def detailed_memory_benchmark(self, model: nn.Module, input_tensor: torch.Tensor, batch_size: int) -> Dict:\n",
    "        \"\"\"Detailed memory and performance benchmark\"\"\"\n",
    "        results = {}\n",
    "        modes = [\"eager\", \"default\", \"reduce-overhead\", \"max-autotune\"]\n",
    "        \n",
    "        for mode in modes:\n",
    "            if mode == \"eager\":\n",
    "                test_model = model\n",
    "            else:\n",
    "                torch._dynamo.reset()\n",
    "                test_model = torch.compile(model, mode=mode)\n",
    "            \n",
    "            # Reset memory tracking\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                with torch.no_grad():\n",
    "                    _ = test_model(input_tensor)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Benchmark execution\n",
    "            exec_time = do_bench(\n",
    "                lambda: test_model(input_tensor),\n",
    "                warmup=50,\n",
    "                rep=200\n",
    "            )\n",
    "            \n",
    "            # Memory measurements\n",
    "            if torch.cuda.is_available():\n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "                current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "            else:\n",
    "                peak_memory = 0.0\n",
    "                current_memory = 0.0\n",
    "            \n",
    "            # Memory efficiency (throughput per GB)\n",
    "            throughput = (batch_size * 1000) / exec_time\n",
    "            memory_efficiency = throughput / max(peak_memory, 0.001)\n",
    "            \n",
    "            results[mode] = {\n",
    "                'execution_time': exec_time,\n",
    "                'peak_memory': peak_memory,\n",
    "                'current_memory': current_memory,\n",
    "                'throughput': throughput,\n",
    "                'memory_efficiency': memory_efficiency\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_memory_analysis(self, memory_results: Dict):\n",
    "        \"\"\"Plot memory analysis results\"\"\"\n",
    "        model_names = list(memory_results.keys())\n",
    "        modes = ['eager', 'default', 'reduce-overhead', 'max-autotune']\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        width = 0.2\n",
    "        x = np.arange(len(model_names))\n",
    "        colors = ['red', 'blue', 'green', 'orange']\n",
    "        \n",
    "        # Execution time\n",
    "        for i, mode in enumerate(modes):\n",
    "            times = [memory_results[model][mode]['execution_time'] for model in model_names]\n",
    "            ax1.bar(x + i * width, times, width, label=mode, color=colors[i], alpha=0.8)\n",
    "        \n",
    "        ax1.set_xlabel('Memory Pattern')\n",
    "        ax1.set_ylabel('Execution Time (ms)')\n",
    "        ax1.set_title('Execution Time by Memory Pattern')\n",
    "        ax1.set_xticks(x + width * 1.5)\n",
    "        ax1.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        # Peak memory usage\n",
    "        for i, mode in enumerate(modes):\n",
    "            memory_usage = [memory_results[model][mode]['peak_memory'] for model in model_names]\n",
    "            ax2.bar(x + i * width, memory_usage, width, label=mode, color=colors[i], alpha=0.8)\n",
    "        \n",
    "        ax2.set_xlabel('Memory Pattern')\n",
    "        ax2.set_ylabel('Peak Memory (GB)')\n",
    "        ax2.set_title('Peak Memory Usage by Pattern')\n",
    "        ax2.set_xticks(x + width * 1.5)\n",
    "        ax2.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=45)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Throughput\n",
    "        for i, mode in enumerate(modes):\n",
    "            throughput = [memory_results[model][mode]['throughput'] for model in model_names]\n",
    "            ax3.bar(x + i * width, throughput, width, label=mode, color=colors[i], alpha=0.8)\n",
    "        \n",
    "        ax3.set_xlabel('Memory Pattern')\n",
    "        ax3.set_ylabel('Throughput (samples/sec)')\n",
    "        ax3.set_title('Throughput by Memory Pattern')\n",
    "        ax3.set_xticks(x + width * 1.5)\n",
    "        ax3.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=45)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # Memory efficiency\n",
    "        for i, mode in enumerate(modes):\n",
    "            efficiency = [memory_results[model][mode]['memory_efficiency'] for model in model_names]\n",
    "            ax4.bar(x + i * width, efficiency, width, label=mode, color=colors[i], alpha=0.8)\n",
    "        \n",
    "        ax4.set_xlabel('Memory Pattern')\n",
    "        ax4.set_ylabel('Samples/sec per GB')\n",
    "        ax4.set_title('Memory Efficiency by Pattern')\n",
    "        ax4.set_xticks(x + width * 1.5)\n",
    "        ax4.set_xticklabels([name.replace('_', '\\n') for name in model_names], rotation=45)\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print memory optimization summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MEMORY OPTIMIZATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            print(f\"\\n{model_name.upper().replace('_', ' ')}:\")\n",
    "            eager_results = memory_results[model_name]['eager']\n",
    "            best_mode = max(modes[1:], key=lambda m: memory_results[model_name][m]['memory_efficiency'])\n",
    "            best_results = memory_results[model_name][best_mode]\n",
    "            \n",
    "            speedup = eager_results['execution_time'] / best_results['execution_time']\n",
    "            memory_reduction = (eager_results['peak_memory'] - best_results['peak_memory']) / eager_results['peak_memory'] * 100\n",
    "            efficiency_gain = best_results['memory_efficiency'] / eager_results['memory_efficiency']\n",
    "            \n",
    "            print(f\"  Best mode: {best_mode}\")\n",
    "            print(f\"  Speedup: {speedup:.2f}x\")\n",
    "            print(f\"  Memory reduction: {memory_reduction:.1f}%\")\n",
    "            print(f\"  Efficiency gain: {efficiency_gain:.2f}x\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "\n",
    "# Run memory pattern analysis\n",
    "memory_analyzer = MemoryPatternAnalyzer()\n",
    "memory_results = memory_analyzer.analyze_memory_patterns()\n",
    "memory_analyzer.plot_memory_analysis(memory_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size Scaling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BatchScalingAnalyzer:\n",
    "    \"\"\"Analyze how torch.compile performance scales with batch size\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def analyze_batch_scaling(self, model: nn.Module, input_shape: Tuple, batch_sizes: List[int] = None):\n",
    "        \"\"\"Analyze performance scaling across different batch sizes\"\"\"\n",
    "        if batch_sizes is None:\n",
    "            batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        scaling_results = {}\n",
    "        modes = [\"eager\", \"default\", \"max-autotune\"]\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Testing batch size: {batch_size}\")\n",
    "            \n",
    "            # Create input tensor\n",
    "            input_tensor = torch.randn(batch_size, *input_shape, device=self.device)\n",
    "            \n",
    "            batch_results = {}\n",
    "            \n",
    "            for mode in modes:\n",
    "                try:\n",
    "                    if mode == \"eager\":\n",
    "                        test_model = model\n",
    "                    else:\n",
    "                        torch._dynamo.reset()\n",
    "                        test_model = torch.compile(model, mode=mode)\n",
    "                    \n",
    "                    # Warmup\n",
    "                    for _ in range(5):\n",
    "                        with torch.no_grad():\n",
    "                            _ = test_model(input_tensor)\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.synchronize()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        torch.cuda.reset_peak_memory_stats()\n",
    "                    \n",
    "                    # Benchmark\n",
    "                    exec_time = do_bench(\n",
    "                        lambda: test_model(input_tensor),\n",
    "                        warmup=20,\n",
    "                        rep=100\n",
    "                    )\n",
    "                    \n",
    "                    # Memory usage\n",
    "                    if torch.cuda.is_available():\n",
    "                        peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "                    else:\n",
    "                        peak_memory = 0.0\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    throughput = (batch_size * 1000) / exec_time  # samples/second\n",
    "                    latency_per_sample = exec_time / batch_size  # ms per sample\n",
    "                    memory_per_sample = peak_memory / batch_size * 1024  # MB per sample\n",
    "                    \n",
    "                    batch_results[mode] = {\n",
    "                        'execution_time': exec_time,\n",
    "                        'throughput': throughput,\n",
    "                        'latency_per_sample': latency_per_sample,\n",
    "                        'peak_memory': peak_memory,\n",
    "                        'memory_per_sample': memory_per_sample\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed for {mode} mode: {e}\")\n",
    "                    batch_results[mode] = None\n",
    "            \n",
    "            scaling_results[batch_size] = batch_results\n",
    "        \n",
    "        return scaling_results\n",
    "    \n",
    "    def plot_scaling_analysis(self, scaling_results: Dict, title: str = \"Batch Size Scaling Analysis\"):\n",
    "        \"\"\"Plot batch size scaling results\"\"\"\n",
    "        batch_sizes = list(scaling_results.keys())\n",
    "        modes = [\"eager\", \"default\", \"max-autotune\"]\n",
    "        colors = ['red', 'blue', 'green']\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Throughput scaling\n",
    "        for i, mode in enumerate(modes):\n",
    "            throughputs = []\n",
    "            valid_batch_sizes = []\n",
    "            \n",
    "            for batch_size in batch_sizes:\n",
    "                if scaling_results[batch_size][mode] is not None:\n",
    "                    throughputs.append(scaling_results[batch_size][mode]['throughput'])\n",
    "                    valid_batch_sizes.append(batch_size)\n",
    "            \n",
    "            if throughputs:\n",
    "                ax1.plot(valid_batch_sizes, throughputs, 'o-', label=mode, color=colors[i], linewidth=2, markersize=6)\n",
    "        \n",
    "        ax1.set_xlabel('Batch Size')\n",
    "        ax1.set_ylabel('Throughput (samples/sec)')\n",
    "        ax1.set_title('Throughput vs Batch Size')\n",
    "        ax1.set_xscale('log', base=2)\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Latency per sample\n",
    "        for i, mode in enumerate(modes):\n",
    "            latencies = []\n",
    "            valid_batch_sizes = []\n",
    "            \n",
    "            for batch_size in batch_sizes:\n",
    "                if scaling_results[batch_size][mode] is not None:\n",
    "                    latencies.append(scaling_results[batch_size][mode]['latency_per_sample'])\n",
    "                    valid_batch_sizes.append(batch_size)\n",
    "            \n",
    "            if latencies:\n",
    "                ax2.plot(valid_batch_sizes, latencies, 'o-', label=mode, color=colors[i], linewidth=2, markersize=6)\n",
    "        \n",
    "        ax2.set_xlabel('Batch Size')\n",
    "        ax2.set_ylabel('Latency per Sample (ms)')\n",
    "        ax2.set_title('Latency per Sample vs Batch Size')\n",
    "        ax2.set_xscale('log', base=2)\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Memory usage\n",
    "        for i, mode in enumerate(modes):\n",
    "            memories = []\n",
    "            valid_batch_sizes = []\n",
    "            \n",
    "            for batch_size in batch_sizes:\n",
    "                if scaling_results[batch_size][mode] is not None:\n",
    "                    memories.append(scaling_results[batch_size][mode]['peak_memory'])\n",
    "                    valid_batch_sizes.append(batch_size)\n",
    "            \n",
    "            if memories:\n",
    "                ax3.plot(valid_batch_sizes, memories, 'o-', label=mode, color=colors[i], linewidth=2, markersize=6)\n",
    "        \n",
    "        ax3.set_xlabel('Batch Size')\n",
    "        ax3.set_ylabel('Peak Memory (GB)')\n",
    "        ax3.set_title('Memory Usage vs Batch Size')\n",
    "        ax3.set_xscale('log', base=2)\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Speedup over eager mode\n",
    "        for i, mode in enumerate(modes[1:], 1):  # Skip eager\n",
    "            speedups = []\n",
    "            valid_batch_sizes = []\n",
    "            \n",
    "            for batch_size in batch_sizes:\n",
    "                if (scaling_results[batch_size]['eager'] is not None and \n",
    "                    scaling_results[batch_size][mode] is not None):\n",
    "                    eager_time = scaling_results[batch_size]['eager']['execution_time']\n",
    "                    compiled_time = scaling_results[batch_size][mode]['execution_time']\n",
    "                    speedup = eager_time / compiled_time\n",
    "                    speedups.append(speedup)\n",
    "                    valid_batch_sizes.append(batch_size)\n",
    "            \n",
    "            if speedups:\n",
    "                ax4.plot(valid_batch_sizes, speedups, 'o-', label=mode, color=colors[i], linewidth=2, markersize=6)\n",
    "        \n",
    "        ax4.set_xlabel('Batch Size')\n",
    "        ax4.set_ylabel('Speedup over Eager (x)')\n",
    "        ax4.set_title('Speedup vs Batch Size')\n",
    "        ax4.set_xscale('log', base=2)\n",
    "        ax4.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print scaling efficiency analysis\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BATCH SCALING EFFICIENCY ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for mode in modes:\n",
    "            print(f\"\\n{mode.upper()} MODE:\")\n",
    "            \n",
    "            # Find optimal batch size for throughput\n",
    "            best_throughput = 0\n",
    "            best_batch_size = 0\n",
    "            \n",
    "            for batch_size in batch_sizes:\n",
    "                if scaling_results[batch_size][mode] is not None:\n",
    "                    throughput = scaling_results[batch_size][mode]['throughput']\n",
    "                    if throughput > best_throughput:\n",
    "                        best_throughput = throughput\n",
    "                        best_batch_size = batch_size\n",
    "            \n",
    "            if best_batch_size > 0:\n",
    "                print(f\"  Optimal batch size for throughput: {best_batch_size}\")\n",
    "                print(f\"  Peak throughput: {best_throughput:.0f} samples/sec\")\n",
    "                \n",
    "                # Scaling efficiency (throughput increase vs batch size increase)\n",
    "                small_batch = min(batch_sizes)\n",
    "                if scaling_results[small_batch][mode] is not None:\n",
    "                    small_throughput = scaling_results[small_batch][mode]['throughput']\n",
    "                    throughput_ratio = best_throughput / small_throughput\n",
    "                    batch_ratio = best_batch_size / small_batch\n",
    "                    efficiency = throughput_ratio / batch_ratio * 100\n",
    "                    print(f\"  Scaling efficiency: {efficiency:.1f}% (ideal would be 100%)\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Example usage with a representative model\n",
    "scaling_analyzer = BatchScalingAnalyzer()\n",
    "\n",
    "# Create a representative model for scaling analysis\n",
    "scaling_model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "# Run scaling analysis\n",
    "scaling_results = scaling_analyzer.analyze_batch_scaling(scaling_model, (1024,), [1, 2, 4, 8, 16, 32, 64, 128])\n",
    "scaling_analyzer.plot_scaling_analysis(scaling_results, \"Deep Neural Network Batch Scaling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}