{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d1d55d-42e3-4445-b6c1-dfd95f7999a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '*.svg': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf torch_compile_debug\n",
    "!rm *.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79b09805-e61c-4be4-aaf0-2dc148b0b736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch._dynamo\n",
    "from torchvision import models\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51c7031-33d1-44ee-8398-3137b5f7d085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# $ y = f(x) = sin^2(x) + cos^2(x)$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "md('''\n",
    "# $ y = f(x) = sin^2(x) + cos^2(x)$\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82568cbb-750b-4ed9-baf2-1cf2c6028c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Optimization problem:\n",
       "### find $w^*$ that $\\displaystyle \\min_{w} f(w)$\n",
       "\n",
       "## Gradient update\n",
       "### $ w_{i+1} = w_{i} - g(\\nabla f(w))$\n",
       "## For SGD\n",
       "### $  g(\\nabla f(w)) = \\alpha*\\nabla f(w)$\n",
       "## Which makes the update for SGD:\n",
       "### $ w_{i+1} = w_{i} - \\alpha*\\nabla f(w)$\n",
       "\n",
       "## Loss function\n",
       "### $loss(w): loss(model(w,batch_{inputs}), batch_{outputs})$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md('''\n",
    "## Optimization problem:\n",
    "### find $w^*$ that $\\displaystyle \\min_{w} f(w)$\n",
    "\n",
    "## Gradient update\n",
    "### $ w_{i+1} = w_{i} - g(\\\\nabla f(w))$\n",
    "## For SGD\n",
    "### $  g(\\\\nabla f(w)) = \\\\alpha*\\\\nabla f(w)$\n",
    "## Which makes the update for SGD:\n",
    "### $ w_{i+1} = w_{i} - \\\\alpha*\\\\nabla f(w)$\n",
    "\n",
    "## Loss function\n",
    "### $loss(w): loss(model(w,batch_{inputs}), batch_{outputs})$ \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b4c1717-2fbe-43f6-86ad-1f12d397d33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# **Forward graph:** $f(x) = sin^2(x)+cos^2(x)$ \n",
       "\n",
       "# **Backward graph:** $\\frac {df(x)}{d\\vec{w}} = f'(x) = 2sin(x)cos(x) + 2cos(x)(-sin(x))$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md('''\n",
    "# **Forward graph:** $f(x) = sin^2(x)+cos^2(x)$ \\n\n",
    "# **Backward graph:** $\\\\frac {df(x)}{d\\\\vec{w}} = f\\'(x) = 2sin(x)cos(x) + 2cos(x)(-sin(x))$\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba8ff94-c457-41fd-b7ff-91ac36872bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "torch.nn.functional.mse_loss(f(x),torch.ones_like(x)) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f80ceb-7e1a-4140-a17c-b6827e6e7461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:279: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "\n",
    "compiled_f = torch.compile(f)\n",
    "torch.nn.functional.mse_loss(compiled_f(x),torch.ones_like(x)) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3a9cf8b-37ef-4b00-9c1b-d21e9c0ed6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_3344/1097326487.py:2, code: return torch.sin(x)**2 + torch.cos(x)**2\n",
      "        sin = torch.sin(x)\n",
      "        pow_1 = sin ** 2;  sin = None\n",
      "        cos = torch.cos(x);  x = None\n",
      "        pow_2 = cos ** 2;  cos = None\n",
      "        add = pow_1 + pow_2;  pow_1 = pow_2 = None\n",
      "        return (add,)\n",
      "        \n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "inspect_backend raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m, in \u001b[0;36minspect_backend\u001b[0;34m(gm, sample_inputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mforward.svg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> 4\u001b[0m     file\u001b[39m.\u001b[39mwrite(FxGraphDrawer(gm,\u001b[39m'\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mget_dot_graph()\u001b[39m.\u001b[39mcreate_svg())\n\u001b[1;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m gm\u001b[39m.\u001b[39mforward\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/graph_drawer.py:325\u001b[0m, in \u001b[0;36mFxGraphDrawer.__init__\u001b[0;34m(self, graph_module, name, ignore_getattr)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, graph_module: torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mGraphModule, name: \u001b[39mstr\u001b[39m, ignore_getattr: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFXGraphDrawer requires the pydot package to be installed. Please install \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    326\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mpydot through your favorite Python package manager.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m compiled_f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(f, backend\u001b[39m=\u001b[39minspect_backend)\n\u001b[1;32m     10\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1000\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m out \u001b[39m=\u001b[39m compiled_f(x)\n\u001b[1;32m     13\u001b[0m md(\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[39m### Graph\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39m![](\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mforward.svg\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m'''\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: inspect_backend raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "def inspect_backend(gm, sample_inputs):\n",
    "    code = gm.print_readable()\n",
    "    with open(\"forward.svg\", \"wb\") as file:\n",
    "        file.write(FxGraphDrawer(gm,'f').get_dot_graph().create_svg())\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f, backend=inspect_backend)\n",
    "\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "out = compiled_f(x)\n",
    "\n",
    "md(f'''\n",
    "### Graph\n",
    "![]({'forward.svg'})\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2518d-1024-4aeb-a991-67de9dc51090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "619928ed-ed3f-45a4-8ef0-783c70bafaed",
   "metadata": {},
   "source": [
    "# AOTAutograd and Aten IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b900c2c1-b83e-44ba-8eb2-c3d71c3bd53e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[1000]):\n",
      "        # File: /tmp/ipykernel_3344/2663716573.py:7, code: return torch.sin(x)**2 + torch.cos(x)**2\n",
      "        sin: f32[1000] = torch.ops.aten.sin.default(primals_1)\n",
      "        pow_1: f32[1000] = torch.ops.aten.pow.Tensor_Scalar(sin, 2)\n",
      "        cos: f32[1000] = torch.ops.aten.cos.default(primals_1)\n",
      "        pow_2: f32[1000] = torch.ops.aten.pow.Tensor_Scalar(cos, 2)\n",
      "        add: f32[1000] = torch.ops.aten.add.Tensor(pow_1, pow_2);  pow_1 = pow_2 = None\n",
      "        return [add, primals_1, sin, cos]\n",
      "        \n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "inspect_backend raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36minspect_backend\u001b[0;34m(gm, sample_inputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Call AOTAutograd\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m gm_forward \u001b[39m=\u001b[39m aot_module_simplified(gm,sample_inputs,\n\u001b[1;32m     28\u001b[0m                                    fw_compiler\u001b[39m=\u001b[39;49mfw,\n\u001b[1;32m     29\u001b[0m                                    bw_compiler\u001b[39m=\u001b[39;49mbw)\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m gm_forward\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2133\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[39mwith\u001b[39;00m track_graph_compiling(aot_config, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 2133\u001b[0m         compiled_fw_func \u001b[39m=\u001b[39m aot_config\u001b[39m.\u001b[39;49mfw_compiler(\n\u001b[1;32m   2134\u001b[0m             fw_module, flat_args_with_views_handled\n\u001b[1;32m   2135\u001b[0m         )\n\u001b[1;32m   2137\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCompiledFunction\u001b[39;00m(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mFunction):\n",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36minspect_backend.<locals>.fw\u001b[0;34m(gm, sample_inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m gm\u001b[39m.\u001b[39mprint_readable()\n\u001b[0;32m---> 13\u001b[0m g \u001b[39m=\u001b[39m FxGraphDrawer(gm, \u001b[39m'\u001b[39;49m\u001b[39mfn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mforward_aot.svg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/graph_drawer.py:325\u001b[0m, in \u001b[0;36mFxGraphDrawer.__init__\u001b[0;34m(self, graph_module, name, ignore_getattr)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, graph_module: torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mGraphModule, name: \u001b[39mstr\u001b[39m, ignore_getattr: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFXGraphDrawer requires the pydot package to be installed. Please install \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    326\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mpydot through your favorite Python package manager.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     38\u001b[0m compiled_f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(f, backend\u001b[39m=\u001b[39minspect_backend)\n\u001b[0;32m---> 39\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmse_loss(compiled_f(x), y)\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: inspect_backend raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "\n",
    "def f(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "def inspect_backend(gm, sample_inputs): \n",
    "    # Forward compiler capture\n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"forward_aot.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    # Backward compiler capture\n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward_aot.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    # Call AOTAutograd\n",
    "    gm_forward = aot_module_simplified(gm,sample_inputs,\n",
    "                                       fw_compiler=fw,\n",
    "                                       bw_compiler=bw)\n",
    "\n",
    "    return gm_forward\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f, backend=inspect_backend)\n",
    "out = torch.nn.functional.mse_loss(compiled_f(x), y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "264ad8cc-6fc2-4e58-98c7-3dcff496ab59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "|![](forward_aot.svg) | < Forward graph <br><br><br> Backward graph >|![](backward_aot.svg)|\n",
       "|---|---|---|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(f'''\n",
    "|![]({'forward_aot.svg'}) | < Forward graph <br><br><br> Backward graph >|![]({'backward_aot.svg'})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4f76f-1461-43d8-a8d4-1d24a7f6d1a8",
   "metadata": {},
   "source": [
    "# Decomposition to Core Aten IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9f0343-65cc-4d7e-8c53-55292b46c87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, primals_1: f32[1000], primals_2: f32[1000]):\n",
      "        # File: /tmp/ipykernel_3344/1876138810.py:8, code: f_x = torch.sin(x)**2 + torch.cos(x)**2\n",
      "        sin: f32[1000] = torch.ops.aten.sin.default(primals_1)\n",
      "        pow_1: f32[1000] = torch.ops.aten.pow.Tensor_Scalar(sin, 2)\n",
      "        cos: f32[1000] = torch.ops.aten.cos.default(primals_1)\n",
      "        pow_2: f32[1000] = torch.ops.aten.pow.Tensor_Scalar(cos, 2)\n",
      "        add: f32[1000] = torch.ops.aten.add.Tensor(pow_1, pow_2);  pow_1 = pow_2 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_3344/1876138810.py:9, code: return torch.nn.functional.mse_loss(f_x, y)\n",
      "        mse_loss: f32[] = torch.ops.aten.mse_loss.default(add, primals_2)\n",
      "        return [mse_loss, sin, primals_2, add, cos, primals_1]\n",
      "        \n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "inspect_backend raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36minspect_backend\u001b[0;34m(gm, sample_inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Invoke AOTAutograd\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m aot_module_simplified(\n\u001b[1;32m     31\u001b[0m     gm,\n\u001b[1;32m     32\u001b[0m     sample_inputs,\n\u001b[1;32m     33\u001b[0m     fw_compiler\u001b[39m=\u001b[39;49mfw,\n\u001b[1;32m     34\u001b[0m     bw_compiler\u001b[39m=\u001b[39;49mbw,\n\u001b[1;32m     35\u001b[0m     decompositions\u001b[39m=\u001b[39;49mdecompositions\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2133\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[39mwith\u001b[39;00m track_graph_compiling(aot_config, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 2133\u001b[0m         compiled_fw_func \u001b[39m=\u001b[39m aot_config\u001b[39m.\u001b[39;49mfw_compiler(\n\u001b[1;32m   2134\u001b[0m             fw_module, flat_args_with_views_handled\n\u001b[1;32m   2135\u001b[0m         )\n\u001b[1;32m   2137\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCompiledFunction\u001b[39;00m(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mFunction):\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m, in \u001b[0;36minspect_backend.<locals>.fw\u001b[0;34m(gm, sample_inputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m gm\u001b[39m.\u001b[39mprint_readable()\n\u001b[0;32m---> 17\u001b[0m g \u001b[39m=\u001b[39m FxGraphDrawer(gm, \u001b[39m'\u001b[39;49m\u001b[39mfn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mforward_decomp.svg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/graph_drawer.py:325\u001b[0m, in \u001b[0;36mFxGraphDrawer.__init__\u001b[0;34m(self, graph_module, name, ignore_getattr)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, graph_module: torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mGraphModule, name: \u001b[39mstr\u001b[39m, ignore_getattr: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFXGraphDrawer requires the pydot package to be installed. Please install \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    326\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mpydot through your favorite Python package manager.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     43\u001b[0m compiled_f \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(f_loss, backend\u001b[39m=\u001b[39minspect_backend)\n\u001b[0;32m---> 44\u001b[0m out \u001b[39m=\u001b[39m compiled_f(x,y)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m md(\u001b[39m'''\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[39m# $MSE = (\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfrac\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m{n}\u001b[39;00m\u001b[39m)(\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mvec\u001b[39m\u001b[39m{y}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mvec\u001b[39m\u001b[39m{x}\u001b[39;00m\u001b[39m)^2$\u001b[39m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m'''\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: inspect_backend raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "\n",
    "def f_loss(x, y):\n",
    "    f_x = torch.sin(x)**2 + torch.cos(x)**2\n",
    "    return torch.nn.functional.mse_loss(f_x, y)\n",
    "\n",
    "# decompositions = core_aten_decompositions() # Use decomposition to Core Aten IR\n",
    "decompositions = {} # Don't use decomposition to Core Aten IR\n",
    "\n",
    "def inspect_backend(gm, sample_inputs): \n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"forward_decomp.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward_decomp.svg\", \"wb\") as file:\n",
    "            file.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=fw,\n",
    "        bw_compiler=bw,\n",
    "        decompositions=decompositions\n",
    "    )\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f_loss, backend=inspect_backend)\n",
    "out = compiled_f(x,y).backward()\n",
    "\n",
    "\n",
    "md('''\n",
    "# $MSE = (\\\\frac{1}{n})(\\\\vec{y}-\\\\vec{x})^2$\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb86a77-d050-4f80-96b0-c23ac12bd1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md(f'''\n",
    "|![]({'forward_decomp.svg'}) | < Forward graph <br><br><br> Backward graph >|![]({'backward_decomp.svg'})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a397ac-69a2-4cbe-af0d-9d3c50e00ece",
   "metadata": {},
   "source": [
    "# Decomposition to prim IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e71d40-6815-4167-862e-50253913e168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
    "from functorch.compile import make_boxed_func\n",
    "from torch._functorch.aot_autograd import aot_module_simplified\n",
    "from torch._decomp import core_aten_decompositions\n",
    "\n",
    "def f_loss(x, y):\n",
    "    f_x = torch.sin(x)**2 + torch.cos(x)**2\n",
    "    return torch.nn.functional.mse_loss(f_x, y)\n",
    "\n",
    "decompositions = core_aten_decompositions()\n",
    "decompositions.update(\n",
    "    torch._decomp.get_decompositions([\n",
    "        torch.ops.aten.sin,\n",
    "        torch.ops.aten.cos,\n",
    "        torch.ops.aten.add,\n",
    "        torch.ops.aten.sub,\n",
    "        torch.ops.aten.mul,\n",
    "        torch.ops.aten.sum,\n",
    "        torch.ops.aten.mean,\n",
    "        torch.ops.aten.pow.Tensor_Scalar,\n",
    "    ])\n",
    ")\n",
    "\n",
    "def inspect_backend(gm, sample_inputs): \n",
    "    def fw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"forward_decomp_prims.svg\", \"wb\") as f:\n",
    "            f.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "    \n",
    "    def bw(gm, sample_inputs):\n",
    "        gm.print_readable()\n",
    "        g = FxGraphDrawer(gm, 'fn')\n",
    "        with open(\"backward_decomp_prims.svg\", \"wb\") as f:\n",
    "            f.write(g.get_dot_graph().create_svg())\n",
    "        return make_boxed_func(gm.forward)\n",
    "\n",
    "    # Invoke AOTAutograd\n",
    "    return aot_module_simplified(\n",
    "        gm,\n",
    "        sample_inputs,\n",
    "        fw_compiler=fw,\n",
    "        bw_compiler=bw,\n",
    "        decompositions=decompositions\n",
    "    )\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f_loss, backend=inspect_backend)\n",
    "out = compiled_f(x,y).backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b79bad-edc7-413d-94be-84ba286841b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md(f'''\n",
    "|![]({'forward_decomp_prims.svg'}) | < Forward graph <br><br><br> Backward graph >|![]({'backward_decomp_prims.svg'})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91ff887d-46de-48d0-85a6-f1888c04e136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-21 21:32:20,363] torch._inductor.debug: [WARNING] model__7_forward_10 debug trace: /workspaces/devspace/pytorch-compile-blogpost/torch_compile_debug/run_2023_06_21_21_31_46_238774-pid_3344/aot_torchinductor/model__7_forward_10.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FX graph to file: /workspaces/devspace/pytorch-compile-blogpost/torch_compile_debug/run_2023_06_21_21_31_46_238774-pid_3344/aot_torchinductor/model__7_forward_10.2/graph_diagram.svg\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:401\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[39mwith\u001b[39;00m config\u001b[39m.\u001b[39mpatch(config_patches):\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mreturn\u001b[39;00m compile_fx(\n\u001b[1;32m    402\u001b[0m             model_,\n\u001b[1;32m    403\u001b[0m             example_inputs_,\n\u001b[1;32m    404\u001b[0m             \u001b[39m# need extra layer of patching as backwards is compiled out of scope\u001b[39;49;00m\n\u001b[1;32m    405\u001b[0m             inner_compile\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpatch(config_patches)(inner_compile),\n\u001b[1;32m    406\u001b[0m         )\n\u001b[1;32m    408\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39m_raise_error_for_testing\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:1713\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[39mif\u001b[39;00m ok:\n\u001b[0;32m-> 1713\u001b[0m         \u001b[39mreturn\u001b[39;00m compiler_fn(flat_fn, leaf_flat_args, aot_config)\n\u001b[1;32m   1715\u001b[0m \u001b[39m# Strategy 2: Duplicate specialize.\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[39m# In Haskell types, suppose you have:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[39m#   }\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39m#   keep_arg_mask = [True, True, False, True]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:2133\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2132\u001b[0m     \u001b[39mwith\u001b[39;00m track_graph_compiling(aot_config, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 2133\u001b[0m         compiled_fw_func \u001b[39m=\u001b[39m aot_config\u001b[39m.\u001b[39;49mfw_compiler(\n\u001b[1;32m   2134\u001b[0m             fw_module, flat_args_with_views_handled\n\u001b[1;32m   2135\u001b[0m         )\n\u001b[1;32m   2137\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCompiledFunction\u001b[39;00m(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mFunction):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:430\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler\u001b[0;34m(model, example_inputs)\u001b[0m\n\u001b[1;32m    429\u001b[0m model \u001b[39m=\u001b[39m convert_outplace_to_inplace(model)\n\u001b[0;32m--> 430\u001b[0m \u001b[39mreturn\u001b[39;00m inner_compile(\n\u001b[1;32m    431\u001b[0m     model,\n\u001b[1;32m    432\u001b[0m     example_inputs,\n\u001b[1;32m    433\u001b[0m     num_fixed\u001b[39m=\u001b[39;49mfixed,\n\u001b[1;32m    434\u001b[0m     cudagraphs\u001b[39m=\u001b[39;49mcudagraphs,\n\u001b[1;32m    435\u001b[0m     graph_id\u001b[39m=\u001b[39;49mgraph_id,\n\u001b[1;32m    436\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/debug_utils.py:595\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m    597\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py:239\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:177\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id)\u001b[0m\n\u001b[1;32m    176\u001b[0m         graph\u001b[39m.\u001b[39mrun(\u001b[39m*\u001b[39mexample_inputs)\n\u001b[0;32m--> 177\u001b[0m         compiled_fn \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49mcompile_to_fn()\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cudagraphs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py:586\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile_to_fn\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_to_module()\u001b[39m.\u001b[39mcall\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py:571\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcodecache\u001b[39;00m \u001b[39mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m--> 571\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcodegen()\n\u001b[1;32m    572\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mdebug:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py:520\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_wrapper_code()\n\u001b[0;32m--> 520\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler \u001b[39m=\u001b[39m Scheduler(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffers)\n\u001b[1;32m    521\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# mypy can't figure this out\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py:639\u001b[0m, in \u001b[0;36mScheduler.__init__\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    638\u001b[0m V\u001b[39m.\u001b[39mdebug\u001b[39m.\u001b[39mir_post_fusion(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes)\n\u001b[0;32m--> 639\u001b[0m V\u001b[39m.\u001b[39;49mdebug\u001b[39m.\u001b[39;49mgraph_diagram(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnodes)\n\u001b[1;32m    640\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug_draw_graph()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py:401\u001b[0m, in \u001b[0;36mDebugFormatter.graph_diagram\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgraph_diagram\u001b[39m(\u001b[39mself\u001b[39m, nodes: SchedulerNodeList):\n\u001b[0;32m--> 401\u001b[0m     draw_buffers(nodes, fname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename(\u001b[39m\"\u001b[39;49m\u001b[39mgraph_diagram.svg\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/debug.py:89\u001b[0m, in \u001b[0;36mdraw_buffers\u001b[0;34m(nodes, print_graph, fname)\u001b[0m\n\u001b[1;32m     88\u001b[0m gm\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mlint()\n\u001b[0;32m---> 89\u001b[0m draw_graph(gm, fname, clear_meta\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/partitioners.py:548\u001b[0m, in \u001b[0;36mdraw_graph\u001b[0;34m(traced, fname, figname, clear_meta)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWriting FX graph to file: \u001b[39m\u001b[39m{\u001b[39;00mbase\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 548\u001b[0m g \u001b[39m=\u001b[39m graph_drawer\u001b[39m.\u001b[39;49mFxGraphDrawer(traced, figname)\n\u001b[1;32m    549\u001b[0m x \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mget_main_dot_graph()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/graph_drawer.py:325\u001b[0m, in \u001b[0;36mFxGraphDrawer.__init__\u001b[0;34m(self, graph_module, name, ignore_getattr)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, graph_module: torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mGraphModule, name: \u001b[39mstr\u001b[39m, ignore_getattr: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFXGraphDrawer requires the pydot package to be installed. Please install \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    326\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mpydot through your favorite Python package manager.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1000\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(x)\n\u001b[0;32m---> 19\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmse_loss(compiled_f(x),y)\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:517\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(random_calls_instructions)\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    506\u001b[0m     stack_values\n\u001b[1;32m    507\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m     \u001b[39m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_output_instructions(\n\u001b[0;32m--> 517\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, \u001b[39mlist\u001b[39;49m(\u001b[39mreversed\u001b[39;49m(stack_values)), root)\n\u001b[1;32m    518\u001b[0m         \u001b[39m+\u001b[39m [create_instruction(\u001b[39m\"\u001b[39m\u001b[39mUNPACK_SEQUENCE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(stack_values))]\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     graph_output_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_var(\u001b[39m\"\u001b[39m\u001b[39mgraph_out\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised RuntimeError: FXGraphDrawer requires the pydot package to be installed. Please install pydot through your favorite Python package manager.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch._dynamo\n",
    "def f(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2 \n",
    "\n",
    "torch._dynamo.reset()\n",
    "compiled_f = torch.compile(f, backend='inductor',\n",
    "                              options={'trace.enabled':True,\n",
    "                                       'trace.graph_diagram':True})\n",
    "\n",
    "\n",
    "# device = 'cpu'\n",
    "device = 'cuda'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(1000, requires_grad=True).to(device)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "out = torch.nn.functional.mse_loss(compiled_f(x),y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06362e9-7783-46bf-8498-bce83bc749ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "fwd = glob.glob('torch_compile_debug/run_*/aot_torchinductor/*forward*/graph_diagram.svg')[-1]\n",
    "bwd = glob.glob('torch_compile_debug/run_*/aot_torchinductor/*backward*/graph_diagram.svg')[-1]\n",
    "\n",
    "md(f'''\n",
    "|![]({fwd}) | < Forward graph <br><br><br> Backward graph >|![]({bwd})|\n",
    "|---|---|---|\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68e773-dce1-431e-8070-e2825d255bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
