{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Graph Optimization Techniques\n",
    "\n",
    "This notebook explores advanced graph optimization techniques beyond basic torch.compile, including custom operators, graph transformations, and manual optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fx import GraphModule, symbolic_trace\n",
    "from torch.fx.passes import shape_prop\n",
    "import torch._dynamo as dynamo\n",
    "from triton.testing import do_bench\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any, Callable\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Operator Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CustomOperatorBuilder:\n",
    "    \"\"\"Builder for creating custom optimized operators\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fused_linear_gelu(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Fused linear + GELU operation\"\"\"\n",
    "        # This would typically be implemented in C++/CUDA for best performance\n",
    "        linear_output = F.linear(input_tensor, weight, bias)\n",
    "        return F.gelu(linear_output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fused_conv_bn_relu(input_tensor: torch.Tensor, conv_weight: torch.Tensor, \n",
    "                          bn_weight: torch.Tensor, bn_bias: torch.Tensor,\n",
    "                          bn_mean: torch.Tensor, bn_var: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
    "        \"\"\"Fused convolution + batch norm + ReLU\"\"\"\n",
    "        # Convolution\n",
    "        conv_out = F.conv2d(input_tensor, conv_weight)\n",
    "        \n",
    "        # Batch normalization\n",
    "        bn_out = F.batch_norm(conv_out, bn_mean, bn_var, bn_weight, bn_bias, \n",
    "                             training=False, eps=eps)\n",
    "        \n",
    "        # ReLU\n",
    "        return F.relu(bn_out)\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimized_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "                           mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"Memory-efficient attention implementation\"\"\"\n",
    "        batch_size, seq_len, d_model = query.shape\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # Scaled dot-product attention with optional masking\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def create_custom_op_model(self, input_dim: int = 512, hidden_dim: int = 1024, output_dim: int = 256):\n",
    "        \"\"\"Create a model using custom operators\"\"\"\n",
    "        \n",
    "        class CustomOpModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.linear1_weight = nn.Parameter(torch.randn(hidden_dim, input_dim))\n",
    "                self.linear1_bias = nn.Parameter(torch.randn(hidden_dim))\n",
    "                self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Use custom fused linear + GELU\n",
    "                x = CustomOperatorBuilder.fused_linear_gelu(x, self.linear1_weight, self.linear1_bias)\n",
    "                x = self.linear2(x)\n",
    "                return x\n",
    "        \n",
    "        return CustomOpModel()\n",
    "    \n",
    "    def benchmark_custom_ops(self, batch_size: int = 64, input_dim: int = 512):\n",
    "        \"\"\"Benchmark custom operators vs standard implementation\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Standard model\n",
    "        standard_model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 256)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Custom operator model\n",
    "        custom_model = self.create_custom_op_model(input_dim).to(device)\n",
    "        \n",
    "        # Test input\n",
    "        input_tensor = torch.randn(batch_size, input_dim, device=device)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Benchmark standard model\n",
    "        print(\"Benchmarking standard model...\")\n",
    "        standard_eager_time = do_bench(lambda: standard_model(input_tensor), warmup=50, rep=200)\n",
    "        standard_compiled = torch.compile(standard_model)\n",
    "        standard_compiled_time = do_bench(lambda: standard_compiled(input_tensor), warmup=50, rep=200)\n",
    "        \n",
    "        results['standard'] = {\n",
    "            'eager': standard_eager_time,\n",
    "            'compiled': standard_compiled_time\n",
    "        }\n",
    "        \n",
    "        # Benchmark custom model\n",
    "        print(\"Benchmarking custom operator model...\")\n",
    "        custom_eager_time = do_bench(lambda: custom_model(input_tensor), warmup=50, rep=200)\n",
    "        custom_compiled = torch.compile(custom_model)\n",
    "        custom_compiled_time = do_bench(lambda: custom_compiled(input_tensor), warmup=50, rep=200)\n",
    "        \n",
    "        results['custom'] = {\n",
    "            'eager': custom_eager_time,\n",
    "            'compiled': custom_compiled_time\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test custom operators\n",
    "custom_op_builder = CustomOperatorBuilder()\n",
    "custom_op_results = custom_op_builder.benchmark_custom_ops()\n",
    "\n",
    "print(\"\\nCustom Operator Benchmark Results:\")\n",
    "print(\"=\"*50)\n",
    "for model_type, times in custom_op_results.items():\n",
    "    eager_time = times['eager']\n",
    "    compiled_time = times['compiled']\n",
    "    speedup = eager_time / compiled_time\n",
    "    print(f\"{model_type.capitalize()} model:\")\n",
    "    print(f\"  Eager: {eager_time:.3f} ms\")\n",
    "    print(f\"  Compiled: {compiled_time:.3f} ms\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Pattern Matching and Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GraphPatternOptimizer:\n",
    "    \"\"\"Advanced graph pattern matching and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_patterns = []\n",
    "        self.applied_optimizations = []\n",
    "    \n",
    "    def register_pattern(self, pattern_name: str, matcher: Callable, replacer: Callable):\n",
    "        \"\"\"Register a custom optimization pattern\"\"\"\n",
    "        self.optimization_patterns.append({\n",
    "            'name': pattern_name,\n",
    "            'matcher': matcher,\n",
    "            'replacer': replacer\n",
    "        })\n",
    "    \n",
    "    def linear_relu_pattern_matcher(self, graph: GraphModule) -> List[Tuple]:\n",
    "        \"\"\"Find Linear -> ReLU patterns in the graph\"\"\"\n",
    "        matches = []\n",
    "        nodes = list(graph.graph.nodes)\n",
    "        \n",
    "        for i, node in enumerate(nodes[:-1]):\n",
    "            next_node = nodes[i + 1]\n",
    "            \n",
    "            # Check for Linear -> ReLU pattern\n",
    "            if (node.op == 'call_function' and \n",
    "                hasattr(node.target, '__name__') and 'linear' in node.target.__name__.lower() and\n",
    "                next_node.op == 'call_function' and\n",
    "                hasattr(next_node.target, '__name__') and 'relu' in next_node.target.__name__.lower()):\n",
    "                \n",
    "                matches.append((node, next_node))\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def fused_linear_relu_replacer(self, graph: GraphModule, linear_node, relu_node):\n",
    "        \"\"\"Replace Linear + ReLU with fused operation\"\"\"\n",
    "        # This is a simplified example - real implementation would be more complex\n",
    "        with graph.graph.inserting_after(linear_node):\n",
    "            # Create a new fused node\n",
    "            fused_node = graph.graph.call_function(\n",
    "                target=CustomOperatorBuilder.fused_linear_gelu,  # Placeholder for fused op\n",
    "                args=linear_node.args\n",
    "            )\n",
    "            \n",
    "            # Replace all uses of relu_node with fused_node\n",
    "            relu_node.replace_all_uses_with(fused_node)\n",
    "        \n",
    "        # Remove the original nodes\n",
    "        graph.graph.erase_node(relu_node)\n",
    "        graph.graph.erase_node(linear_node)\n",
    "        \n",
    "        return fused_node\n",
    "    \n",
    "    def analyze_graph_structure(self, model: nn.Module, input_tensor: torch.Tensor) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze the structure of a model's computation graph\"\"\"\n",
    "        # Trace the model to get FX graph\n",
    "        traced_model = symbolic_trace(model)\n",
    "        \n",
    "        # Add shape information\n",
    "        shape_prop.ShapeProp(traced_model).propagate(input_tensor)\n",
    "        \n",
    "        nodes = list(traced_model.graph.nodes)\n",
    "        \n",
    "        analysis = {\n",
    "            'total_nodes': len(nodes),\n",
    "            'node_types': {},\n",
    "            'optimization_opportunities': [],\n",
    "            'memory_usage_estimate': 0,\n",
    "            'compute_intensity': 0\n",
    "        }\n",
    "        \n",
    "        # Analyze node types\n",
    "        for node in nodes:\n",
    "            node_type = f\"{node.op}:{getattr(node.target, '__name__', str(node.target))}\"\n",
    "            analysis['node_types'][node_type] = analysis['node_types'].get(node_type, 0) + 1\n",
    "        \n",
    "        # Look for optimization opportunities\n",
    "        linear_relu_matches = self.linear_relu_pattern_matcher(traced_model)\n",
    "        if linear_relu_matches:\n",
    "            analysis['optimization_opportunities'].append({\n",
    "                'pattern': 'Linear -> ReLU fusion',\n",
    "                'count': len(linear_relu_matches),\n",
    "                'potential_speedup': '10-20%'\n",
    "            })\n",
    "        \n",
    "        # Estimate memory usage and compute intensity\n",
    "        for node in nodes:\n",
    "            if hasattr(node, 'meta') and 'tensor_meta' in node.meta:\n",
    "                tensor_meta = node.meta['tensor_meta']\n",
    "                if hasattr(tensor_meta, 'shape') and hasattr(tensor_meta, 'dtype'):\n",
    "                    # Rough memory estimate\n",
    "                    numel = 1\n",
    "                    for dim in tensor_meta.shape:\n",
    "                        numel *= dim\n",
    "                    \n",
    "                    dtype_size = 4 if tensor_meta.dtype in [torch.float32, torch.int32] else 2\n",
    "                    analysis['memory_usage_estimate'] += numel * dtype_size\n",
    "        \n",
    "        # Convert bytes to MB\n",
    "        analysis['memory_usage_estimate'] /= (1024 * 1024)\n",
    "        \n",
    "        return analysis, traced_model\n",
    "    \n",
    "    def apply_graph_optimizations(self, model: nn.Module, input_tensor: torch.Tensor) -> Tuple[nn.Module, Dict]:\n",
    "        \"\"\"Apply registered optimization patterns to a model\"\"\"\n",
    "        analysis, traced_model = self.analyze_graph_structure(model, input_tensor)\n",
    "        \n",
    "        optimizations_applied = []\n",
    "        \n",
    "        # Apply each registered pattern\n",
    "        for pattern in self.optimization_patterns:\n",
    "            matches = pattern['matcher'](traced_model)\n",
    "            \n",
    "            if matches:\n",
    "                print(f\"Applying optimization: {pattern['name']} ({len(matches)} matches)\")\n",
    "                \n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        pattern['replacer'](traced_model, *match)\n",
    "                        optimizations_applied.append(pattern['name'])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to apply {pattern['name']}: {e}\")\n",
    "        \n",
    "        # Recompile the graph\n",
    "        traced_model.recompile()\n",
    "        \n",
    "        optimization_summary = {\n",
    "            'original_analysis': analysis,\n",
    "            'applied_optimizations': optimizations_applied,\n",
    "            'optimization_count': len(optimizations_applied)\n",
    "        }\n",
    "        \n",
    "        return traced_model, optimization_summary\n",
    "    \n",
    "    def visualize_graph(self, model: nn.Module, input_tensor: torch.Tensor, title: str = \"Model Graph\"):\n",
    "        \"\"\"Visualize the computation graph\"\"\"\n",
    "        traced_model = symbolic_trace(model)\n",
    "        \n",
    "        # Get graph representation\n",
    "        graph_str = str(traced_model.graph)\n",
    "        nodes = list(traced_model.graph.nodes)\n",
    "        \n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"=\" * len(title))\n",
    "        print(f\"Total nodes: {len(nodes)}\")\n",
    "        print(\"\\nGraph structure:\")\n",
    "        \n",
    "        for i, node in enumerate(nodes):\n",
    "            indent = \"  \" * (i % 3)  # Simple indentation for readability\n",
    "            node_info = f\"{node.name}: {node.op}\"\n",
    "            if hasattr(node, 'target') and hasattr(node.target, '__name__'):\n",
    "                node_info += f\" -> {node.target.__name__}\"\n",
    "            elif hasattr(node, 'target'):\n",
    "                node_info += f\" -> {str(node.target)[:50]}...\"\n",
    "            \n",
    "            print(f\"{indent}{i:2d}. {node_info}\")\n",
    "        \n",
    "        return traced_model\n",
    "\n",
    "# Example usage\n",
    "graph_optimizer = GraphPatternOptimizer()\n",
    "\n",
    "# Register optimization patterns\n",
    "graph_optimizer.register_pattern(\n",
    "    \"linear_relu_fusion\",\n",
    "    graph_optimizer.linear_relu_pattern_matcher,\n",
    "    graph_optimizer.fused_linear_relu_replacer\n",
    ")\n",
    "\n",
    "# Create a test model\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "input_tensor = torch.randn(32, 512)\n",
    "\n",
    "# Analyze original model\n",
    "print(\"Original Model Analysis:\")\n",
    "analysis, traced_model = graph_optimizer.analyze_graph_structure(test_model, input_tensor)\n",
    "\n",
    "print(f\"Total nodes: {analysis['total_nodes']}\")\n",
    "print(f\"Memory estimate: {analysis['memory_usage_estimate']:.2f} MB\")\n",
    "print(\"\\nOptimization opportunities:\")\n",
    "for opp in analysis['optimization_opportunities']:\n",
    "    print(f\"  - {opp['pattern']}: {opp['count']} instances, {opp['potential_speedup']} improvement\")\n",
    "\n",
    "# Visualize the graph\n",
    "graph_optimizer.visualize_graph(test_model, input_tensor, \"Original Model Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Compilation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AdvancedCompilationStrategies:\n",
    "    \"\"\"Implement and benchmark advanced compilation strategies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.compilation_strategies = {}\n",
    "    \n",
    "    def register_custom_backend(self, name: str, backend_fn: Callable):\n",
    "        \"\"\"Register a custom compilation backend\"\"\"\n",
    "        self.compilation_strategies[name] = backend_fn\n",
    "    \n",
    "    def shape_specialized_compilation(self, model: nn.Module, input_shapes: List[Tuple]):\n",
    "        \"\"\"Compile model with shape specialization for different input sizes\"\"\"\n",
    "        specialized_models = {}\n",
    "        \n",
    "        for i, shape in enumerate(input_shapes):\n",
    "            print(f\"Compiling for shape {shape}...\")\n",
    "            \n",
    "            # Create sample input\n",
    "            sample_input = torch.randn(*shape, device=self.device)\n",
    "            \n",
    "            # Compile with shape specialization\n",
    "            dynamo.reset()\n",
    "            specialized_model = torch.compile(\n",
    "                model,\n",
    "                mode=\"max-autotune\",\n",
    "                dynamic=False  # Force static shapes\n",
    "            )\n",
    "            \n",
    "            # Warm up the compiled model\n",
    "            for _ in range(5):\n",
    "                with torch.no_grad():\n",
    "                    _ = specialized_model(sample_input)\n",
    "            \n",
    "            specialized_models[shape] = specialized_model\n",
    "        \n",
    "        return specialized_models\n",
    "    \n",
    "    def dynamic_shape_compilation(self, model: nn.Module, min_shape: Tuple, max_shape: Tuple):\n",
    "        \"\"\"Compile model with dynamic shape support\"\"\"\n",
    "        print(f\"Compiling with dynamic shapes: {min_shape} to {max_shape}\")\n",
    "        \n",
    "        # Mark dynamic dimensions\n",
    "        sample_input = torch.randn(*max_shape, device=self.device)\n",
    "        \n",
    "        # Compile with dynamic shapes enabled\n",
    "        dynamo.reset()\n",
    "        dynamic_model = torch.compile(\n",
    "            model,\n",
    "            mode=\"reduce-overhead\",\n",
    "            dynamic=True\n",
    "        )\n",
    "        \n",
    "        # Warm up with different shapes\n",
    "        for batch_size in [min_shape[0], (min_shape[0] + max_shape[0]) // 2, max_shape[0]]:\n",
    "            test_input = torch.randn(batch_size, *min_shape[1:], device=self.device)\n",
    "            with torch.no_grad():\n",
    "                _ = dynamic_model(test_input)\n",
    "        \n",
    "        return dynamic_model\n",
    "    \n",
    "    def profile_guided_optimization(self, model: nn.Module, profiling_inputs: List[torch.Tensor]):\n",
    "        \"\"\"Use profiling data to guide optimization decisions\"\"\"\n",
    "        print(\"Running profile-guided optimization...\")\n",
    "        \n",
    "        # Collect profiling data\n",
    "        profile_data = {}\n",
    "        \n",
    "        with torch.profiler.profile(\n",
    "            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=True\n",
    "        ) as prof:\n",
    "            for input_tensor in profiling_inputs:\n",
    "                with torch.no_grad():\n",
    "                    _ = model(input_tensor)\n",
    "        \n",
    "        # Analyze profiling results\n",
    "        key_averages = prof.key_averages(group_by_input_shape=True)\n",
    "        \n",
    "        # Find bottlenecks\n",
    "        bottlenecks = []\n",
    "        for avg in key_averages:\n",
    "            if avg.cuda_time_total > 1000:  # Operations taking more than 1ms\n",
    "                bottlenecks.append({\n",
    "                    'name': avg.key,\n",
    "                    'cuda_time': avg.cuda_time_total,\n",
    "                    'cpu_time': avg.cpu_time_total,\n",
    "                    'count': avg.count\n",
    "                })\n",
    "        \n",
    "        profile_data['bottlenecks'] = sorted(bottlenecks, key=lambda x: x['cuda_time'], reverse=True)\n",
    "        \n",
    "        # Compile with profile-guided optimizations\n",
    "        if profile_data['bottlenecks']:\n",
    "            # Use max-autotune for models with identified bottlenecks\n",
    "            optimized_model = torch.compile(model, mode=\"max-autotune\")\n",
    "        else:\n",
    "            # Use default mode for simpler models\n",
    "            optimized_model = torch.compile(model, mode=\"default\")\n",
    "        \n",
    "        return optimized_model, profile_data\n",
    "    \n",
    "    def multi_stage_compilation(self, model: nn.Module, input_tensor: torch.Tensor):\n",
    "        \"\"\"Apply multi-stage compilation with different optimization levels\"\"\"\n",
    "        compilation_stages = {\n",
    "            'stage1_fast': {'mode': 'default', 'description': 'Fast compilation'},\n",
    "            'stage2_balanced': {'mode': 'reduce-overhead', 'description': 'Balanced optimization'},\n",
    "            'stage3_aggressive': {'mode': 'max-autotune', 'description': 'Aggressive optimization'}\n",
    "        }\n",
    "        \n",
    "        stage_results = {}\n",
    "        \n",
    "        for stage_name, config in compilation_stages.items():\n",
    "            print(f\"Running {stage_name}: {config['description']}\")\n",
    "            \n",
    "            # Reset compilation state\n",
    "            dynamo.reset()\n",
    "            \n",
    "            # Measure compilation time\n",
    "            compile_start = time.perf_counter()\n",
    "            compiled_model = torch.compile(model, mode=config['mode'])\n",
    "            compile_time = time.perf_counter() - compile_start\n",
    "            \n",
    "            # Measure first execution time (includes lazy compilation)\n",
    "            first_exec_start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                output = compiled_model(input_tensor)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            first_exec_time = time.perf_counter() - first_exec_start\n",
    "            \n",
    "            # Measure steady-state performance\n",
    "            steady_state_time = do_bench(\n",
    "                lambda: compiled_model(input_tensor),\n",
    "                warmup=20,\n",
    "                rep=100\n",
    "            )\n",
    "            \n",
    "            stage_results[stage_name] = {\n",
    "                'compilation_time': compile_time * 1000,  # ms\n",
    "                'first_execution_time': first_exec_time * 1000,  # ms\n",
    "                'steady_state_time': steady_state_time,  # ms\n",
    "                'total_setup_time': (compile_time + first_exec_time) * 1000,  # ms\n",
    "                'mode': config['mode'],\n",
    "                'description': config['description']\n",
    "            }\n",
    "        \n",
    "        return stage_results\n",
    "    \n",
    "    def benchmark_compilation_strategies(self, model: nn.Module, input_shapes: List[Tuple]):\n",
    "        \"\"\"Comprehensive benchmark of different compilation strategies\"\"\"\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        benchmark_results = {}\n",
    "        \n",
    "        # 1. Standard compilation modes\n",
    "        print(\"\\n1. Testing standard compilation modes...\")\n",
    "        standard_input = torch.randn(*input_shapes[0], device=self.device)\n",
    "        standard_results = self.multi_stage_compilation(model, standard_input)\n",
    "        benchmark_results['standard_modes'] = standard_results\n",
    "        \n",
    "        # 2. Shape-specialized compilation\n",
    "        print(\"\\n2. Testing shape-specialized compilation...\")\n",
    "        specialized_models = self.shape_specialized_compilation(model, input_shapes[:3])\n",
    "        \n",
    "        shape_specialized_results = {}\n",
    "        for shape, specialized_model in specialized_models.items():\n",
    "            test_input = torch.randn(*shape, device=self.device)\n",
    "            exec_time = do_bench(lambda: specialized_model(test_input), warmup=20, rep=100)\n",
    "            shape_specialized_results[str(shape)] = exec_time\n",
    "        \n",
    "        benchmark_results['shape_specialized'] = shape_specialized_results\n",
    "        \n",
    "        # 3. Dynamic shape compilation\n",
    "        print(\"\\n3. Testing dynamic shape compilation...\")\n",
    "        min_shape = min(input_shapes, key=lambda x: x[0])\n",
    "        max_shape = max(input_shapes, key=lambda x: x[0])\n",
    "        \n",
    "        dynamic_model = self.dynamic_shape_compilation(model, min_shape, max_shape)\n",
    "        \n",
    "        dynamic_results = {}\n",
    "        for shape in input_shapes[:3]:\n",
    "            test_input = torch.randn(*shape, device=self.device)\n",
    "            exec_time = do_bench(lambda: dynamic_model(test_input), warmup=20, rep=100)\n",
    "            dynamic_results[str(shape)] = exec_time\n",
    "        \n",
    "        benchmark_results['dynamic_shapes'] = dynamic_results\n",
    "        \n",
    "        # 4. Profile-guided optimization\n",
    "        print(\"\\n4. Testing profile-guided optimization...\")\n",
    "        profiling_inputs = [torch.randn(*shape, device=self.device) for shape in input_shapes[:2]]\n",
    "        pgo_model, profile_data = self.profile_guided_optimization(model, profiling_inputs)\n",
    "        \n",
    "        pgo_time = do_bench(lambda: pgo_model(standard_input), warmup=20, rep=100)\n",
    "        benchmark_results['profile_guided'] = {\n",
    "            'execution_time': pgo_time,\n",
    "            'bottlenecks_found': len(profile_data['bottlenecks'])\n",
    "        }\n",
    "        \n",
    "        return benchmark_results\n",
    "    \n",
    "    def plot_strategy_comparison(self, benchmark_results: Dict):\n",
    "        \"\"\"Plot comparison of compilation strategies\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Standard modes comparison\n",
    "        standard_modes = benchmark_results['standard_modes']\n",
    "        modes = list(standard_modes.keys())\n",
    "        setup_times = [standard_modes[mode]['total_setup_time'] for mode in modes]\n",
    "        steady_times = [standard_modes[mode]['steady_state_time'] for mode in modes]\n",
    "        \n",
    "        x = np.arange(len(modes))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax1.bar(x - width/2, setup_times, width, label='Setup Time', alpha=0.8)\n",
    "        ax1.bar(x + width/2, steady_times, width, label='Execution Time', alpha=0.8)\n",
    "        ax1.set_xlabel('Compilation Mode')\n",
    "        ax1.set_ylabel('Time (ms)')\n",
    "        ax1.set_title('Standard Compilation Modes')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels([mode.replace('stage', '').replace('_', '\\n') for mode in modes], rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        # 2. Shape specialization vs dynamic shapes\n",
    "        if 'shape_specialized' in benchmark_results and 'dynamic_shapes' in benchmark_results:\n",
    "            specialized_data = benchmark_results['shape_specialized']\n",
    "            dynamic_data = benchmark_results['dynamic_shapes']\n",
    "            \n",
    "            shapes = list(specialized_data.keys())\n",
    "            specialized_times = list(specialized_data.values())\n",
    "            dynamic_times = [dynamic_data[shape] for shape in shapes if shape in dynamic_data]\n",
    "            \n",
    "            x = np.arange(len(shapes))\n",
    "            ax2.bar(x - width/2, specialized_times, width, label='Shape Specialized', alpha=0.8)\n",
    "            ax2.bar(x + width/2, dynamic_times, width, label='Dynamic Shapes', alpha=0.8)\n",
    "            ax2.set_xlabel('Input Shape')\n",
    "            ax2.set_ylabel('Execution Time (ms)')\n",
    "            ax2.set_title('Shape Specialized vs Dynamic Compilation')\n",
    "            ax2.set_xticks(x)\n",
    "            ax2.set_xticklabels([shape.replace('(', '').replace(')', '').replace(', ', '\\nx') for shape in shapes], rotation=45)\n",
    "            ax2.legend()\n",
    "        \n",
    "        # 3. Compilation overhead analysis\n",
    "        compile_times = [standard_modes[mode]['compilation_time'] for mode in modes]\n",
    "        first_exec_times = [standard_modes[mode]['first_execution_time'] for mode in modes]\n",
    "        \n",
    "        ax3.bar(x - width/2, compile_times, width, label='Compilation', alpha=0.8)\n",
    "        ax3.bar(x + width/2, first_exec_times, width, label='First Execution', alpha=0.8)\n",
    "        ax3.set_xlabel('Compilation Mode')\n",
    "        ax3.set_ylabel('Time (ms)')\n",
    "        ax3.set_title('Compilation Overhead Breakdown')\n",
    "        ax3.set_xticks(x)\n",
    "        ax3.set_xticklabels([mode.replace('stage', '').replace('_', '\\n') for mode in modes], rotation=45)\n",
    "        ax3.legend()\n",
    "        ax3.set_yscale('log')\n",
    "        \n",
    "        # 4. Overall strategy comparison\n",
    "        strategy_names = ['Standard', 'Specialized', 'Dynamic', 'PGO']\n",
    "        strategy_times = [\n",
    "            standard_modes['stage2_balanced']['steady_state_time'],\n",
    "            min(benchmark_results['shape_specialized'].values()) if 'shape_specialized' in benchmark_results else 0,\n",
    "            min(benchmark_results['dynamic_shapes'].values()) if 'dynamic_shapes' in benchmark_results else 0,\n",
    "            benchmark_results['profile_guided']['execution_time'] if 'profile_guided' in benchmark_results else 0\n",
    "        ]\n",
    "        \n",
    "        bars = ax4.bar(strategy_names, strategy_times, alpha=0.8, color=['blue', 'green', 'orange', 'red'])\n",
    "        ax4.set_ylabel('Execution Time (ms)')\n",
    "        ax4.set_title('Strategy Comparison (Best Case)')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, time in zip(bars, strategy_times):\n",
    "            if time > 0:\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(strategy_times)*0.01,\n",
    "                        f'{time:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "compilation_strategies = AdvancedCompilationStrategies()\n",
    "\n",
    "# Create a representative model\n",
    "strategy_test_model = nn.Sequential(\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "# Test different input shapes\n",
    "test_shapes = [(16, 1024), (32, 1024), (64, 1024), (128, 1024)]\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "strategy_results = compilation_strategies.benchmark_compilation_strategies(strategy_test_model, test_shapes)\n",
    "\n",
    "# Plot results\n",
    "compilation_strategies.plot_strategy_comparison(strategy_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}