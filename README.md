# pytorch-examples
A repository of PyTorch example

https://medium.com/towards-data-science/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26

https://shashankprasanna.com/


https://rocm.blogs.amd.com/artificial-intelligence/torch_compile/README.html

Accelerate PyTorch Models using torch.compile on AMD GPUs with ROCm
Introduction
PyTorch 2.0 introduces torch.compile(), a tool to vastly accelerate PyTorch code and models. By converting PyTorch code into highly optimized kernels, torch.compile delivers substantial performance improvements with minimal changes to the existing codebase. This feature allows for precise optimization of individual functions, entire modules, and complex training loops, providing a versatile and powerful tool for enhancing computational efficiency.

In this blog, we demonstrate how torch.compile speeds up various real-world models on AMD GPUs with ROCm.

How torch.compile works
The execution of torch.compile involves several crucial steps:

Graph acquisition: The model is broken down and rewritten as subgraphs. Subgraphs that can be compiled or optimized are flattened. Subgraphs that can’t be compiled fall back to eager mode.

Graph lowering: All PyTorch operations are decomposed into their chosen backend-specific kernels.

Graph compilation: All the backend kernels call their corresponding low-level device operations.

Four essential technologies drive torch.compile: TorchDynamo, AOTAutograd, PrimTorch, and TorchInductor. Each of these components plays a crucial role in enabling the functionality of torch.compile.

TorchDynamo: It acquires graphs reliably and fast. TorchDynamo works by interpreting Python bytecode symbolically, converting it into a graph of tensor operations. If it comes across a segment of code that it cannot interpret, it defaults to the regular Python interpreter. This approach ensures that it can handle a wide range of programs while providing significant performance improvements.

AOT Autograd: It reuses Autograd for Ahead-of-Time (AoT) graphs. AOT Autograd is the automatic differentiation engine in PyTorch 2.0. Its function is to produce backward traces in an ahead-of-time fashion, enhancing the efficiency of the differentiation process. AOT Autograd uses PyTorch’s torch_dispatch mechanism to trace through the existing PyTorch autograd engine, capturing the backward pass ahead of time. This enables acceleration of both the forward and backward pass.

PrimTorch: It provides stable primitive operators. It decomposes complicated PyTorch operations into simpler ones.

TorchInductor: It generates high-speed code for accelerators and backends. TorchInductor is a deep-learning compiler that translates intermediate representations into executable code. It takes the computation graph generated by TorchDynamo and converts it into optimized low-level kernels. For NVIDIA and AMD GPUs, it employs OpenAI Triton as a fundamental component.

The torch.compile function comes with multiple modes for compiling, e.g., default, reduce-overhead, and max-autotune, which essentially differ in compilation time and inference overhead. In general, max-autotune takes longer than reduce-overhead for compilation but results in faster inference. The default mode is the fastest for compilation but it is not as efficient compared to reduce-overhead for inference time. The torch.compile function compiles the model into an optimized kernel during the first execution. Therefore, the initial run may take longer due to compilation, but subsequent executions demonstrate speedups due to reduced Python overhead and GPU reads and writes. The resulting speedup can vary based on model architecture and batch size. You can read more about the PyTorch compilation process in PyTorch 2.0 Introduction presentation and tutorial.

In this blog, we demonstrate that using torch.compile can speed up real-world models on AMD GPU with ROCm by evaluating the performance of various models in Eager-mode and different modes of torch.compile.

Image classification with convolutional neural network (ResNet-152) model

Image classification with vision transformer model

Text generation with Llama 2 7B model

You can find the complete code used in this blog in the ROCm blogs repository.
